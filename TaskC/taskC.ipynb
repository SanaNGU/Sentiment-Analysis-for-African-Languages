{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## methods\n",
    "* M1: use similar data from task A with model sutible for it\n",
    "* M2: use BT\n",
    "* M3: adapters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-17 21:00:11.239411: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-17 21:00:11.239443: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at setu4993/LaBSE and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|█████████████████████████████████████████████| 6/6 [00:00<00:00,  8.70ba/s]\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 19.22ba/s]\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, ID, tweet.\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 5984\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 561\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msana-ltu\u001b[0m (\u001b[33mml_ltu\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.9 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/sanala/Juputer try/afrisenti/TaskC/wandb/run-20230117_210032-rf7y0sbq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtrainerfile\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ml_ltu/huggingface\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ml_ltu/huggingface/runs/rf7y0sbq\u001b[0m\n",
      "{'loss': 0.9233, 'learning_rate': 4.10873440285205e-05, 'epoch': 0.53}          \n",
      " 18%|███████▎                                 | 100/561 [00:40<04:33,  1.69it/s]"
     ]
    }
   ],
   "source": [
    "!python run_train.py  --model_name  'castorini/afriberta_large'   --learning_rate 5e-5 --epoch 5.0  --output_dir 'castorini/afriberta_large-multi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-30 13:28:26.164660: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-30 13:28:26.164694: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2096\n",
      "  Batch size = 8\n",
      " 98%|████████████████████████████████████████ | 256/262 [00:04<00:00, 59.52it/s]Data directory found.\n",
      "100%|█████████████████████████████████████████| 262/262 [00:04<00:00, 53.34it/s]\n"
     ]
    }
   ],
   "source": [
    "!python run_predict_taskC.py --model_name 'castorini/afriberta_large' --language 'or' --output_dir 'castorini/afriberta_large-multi' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-30 13:28:47.653790: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-30 13:28:47.653858: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2096\n",
      "  Batch size = 8\n",
      " 99%|████████████████████████████████████████▌| 259/262 [00:05<00:00, 50.22it/s]Data directory found.\n",
      "100%|█████████████████████████████████████████| 262/262 [00:05<00:00, 43.76it/s]\n"
     ]
    }
   ],
   "source": [
    "!python run_predict_taskC.py --model_name 'Davlan/afro-xlmr-base' --language 'or' --output_dir 'Davlan/afro-xlmr-base-multi' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-17 21:28:21.931294: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-17 21:28:21.931324: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at setu4993/LaBSE and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  6.98ba/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 29.42ba/s]\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, ID, __index_level_0__.\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1810\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 171\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msana-ltu\u001b[0m (\u001b[33mml_ltu\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.9 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/sanala/Juputer try/afrisenti/TaskC/wandb/run-20230117_212841-30zd99aq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtrainerfile\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ml_ltu/huggingface\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ml_ltu/huggingface/runs/30zd99aq\u001b[0m\n",
      "{'loss': 0.7386, 'learning_rate': 2.0760233918128656e-05, 'epoch': 1.75}        \n",
      "100%|█████████████████████████████████████████| 171/171 [01:05<00:00,  2.89it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 73.2326, 'train_samples_per_second': 74.147, 'train_steps_per_second': 2.335, 'train_loss': 0.6207448278951366, 'epoch': 3.0}\n",
      "100%|█████████████████████████████████████████| 171/171 [01:05<00:00,  2.60it/s]\n",
      "Saving model checkpoint to setu4993/LaBSE-sw\n",
      "Configuration saved in setu4993/LaBSE-sw/config.json\n",
      "Model weights saved in setu4993/LaBSE-sw/pytorch_model.bin\n",
      "tokenizer config file saved in setu4993/LaBSE-sw/tokenizer_config.json\n",
      "Special tokens file saved in setu4993/LaBSE-sw/special_tokens_map.json\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \\ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 3.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 171\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 2e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.7386\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 558088225884000.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.62074\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 73.2326\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 74.147\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 2.335\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mtrainerfile\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/ml_ltu/huggingface/runs/30zd99aq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230117_212841-30zd99aq/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-17 21:30:08.558135: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-17 21:30:08.558168: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 396\n",
      "  Batch size = 8\n",
      "100%|███████████████████████████████████████████| 50/50 [00:00<00:00, 54.15it/s]Data directory found.\n",
      "Creating submission files directory.\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 36.60it/s]\n"
     ]
    }
   ],
   "source": [
    "!python run_predict_taskC.py --model_name 'setu4993/LaBSE' --language 'or' --output_dir 'setu4993/LaBSE-sw' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-17 20:51:22.007037: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-17 20:51:22.007067: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Downloading: 100%|██████████████████████████████| 683/683 [00:00<00:00, 583kB/s]\n",
      "Downloading: 100%|██████████████████████████████| 356/356 [00:00<00:00, 344kB/s]\n",
      "Downloading: 100%|█████████████████████████| 4.83M/4.83M [00:00<00:00, 16.1MB/s]\n",
      "Downloading: 100%|██████████████████████████████| 239/239 [00:00<00:00, 187kB/s]\n",
      "Downloading: 100%|█████████████████████████| 1.04G/1.04G [00:16<00:00, 66.6MB/s]\n",
      "Some weights of the model checkpoint at Davlan/xlm-roberta-base-finetuned-amharic were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at Davlan/xlm-roberta-base-finetuned-amharic and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|█████████████████████████████████████████████| 6/6 [00:00<00:00,  8.26ba/s]\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 18.14ba/s]\n",
      "The following columns in the training set  don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, tweet, ID.\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 5984\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 561\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msana-ltu\u001b[0m (\u001b[33mml_ltu\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.9 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/sanala/Juputer try/afrisenti/TaskC/wandb/run-20230117_205201-rq9tsgyj\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtrainerfile\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ml_ltu/huggingface\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ml_ltu/huggingface/runs/rq9tsgyj\u001b[0m\n",
      "{'loss': 0.9688, 'learning_rate': 4.10873440285205e-05, 'epoch': 0.53}          \n",
      "{'loss': 1.0042, 'learning_rate': 3.2174688057041004e-05, 'epoch': 1.07}        \n",
      "{'loss': 1.0364, 'learning_rate': 2.32620320855615e-05, 'epoch': 1.6}           \n",
      "{'loss': 1.0286, 'learning_rate': 1.4349376114081997e-05, 'epoch': 2.14}        \n",
      "{'loss': 1.0346, 'learning_rate': 5.436720142602496e-06, 'epoch': 2.67}         \n",
      " 89%|████████████████████████████████████▌    | 500/561 [03:04<00:26,  2.33it/s]Saving model checkpoint to trainerfile/checkpoint-500\n",
      "Configuration saved in trainerfile/checkpoint-500/config.json\n",
      "Model weights saved in trainerfile/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in trainerfile/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in trainerfile/checkpoint-500/special_tokens_map.json\n",
      "100%|█████████████████████████████████████████| 561/561 [03:34<00:00,  2.70it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 222.55, 'train_samples_per_second': 80.665, 'train_steps_per_second': 2.521, 'train_loss': 1.0155446465640146, 'epoch': 3.0}\n",
      "100%|█████████████████████████████████████████| 561/561 [03:34<00:00,  2.62it/s]\n",
      "Saving model checkpoint to Davlan/xlm-roberta-base-finetuned-amharic-am\n",
      "Configuration saved in Davlan/xlm-roberta-base-finetuned-amharic-am/config.json\n",
      "Model weights saved in Davlan/xlm-roberta-base-finetuned-amharic-am/pytorch_model.bin\n",
      "tokenizer config file saved in Davlan/xlm-roberta-base-finetuned-amharic-am/tokenizer_config.json\n",
      "Special tokens file saved in Davlan/xlm-roberta-base-finetuned-amharic-am/special_tokens_map.json\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \\ 0.488 MB of 0.488 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▃▄▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▃▄▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate █▆▄▃▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss ▁▅█▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 3.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 561\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 1e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 1.0346\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 1845082841817600.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 1.01554\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 222.55\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 80.665\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 2.521\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mtrainerfile\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/ml_ltu/huggingface/runs/rq9tsgyj\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230117_205201-rq9tsgyj/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python run_train.py  --model_name  'Davlan/xlm-roberta-base-finetuned-amharic'  --language 'am' --learning_rate 5e-5 --epoch 3.0  --output_dir 'Davlan/xlm-roberta-base-finetuned-amharic-am'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run_predict_taskC.py --model_name 'Davlan/xlm-roberta-base-finetuned-amharic' --language 'tg' --output_dir 'Davlan/xlm-roberta-base-finetuned-amharic-am' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-18 09:14:46.110832: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-18 09:14:46.110868: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at setu4993/LaBSE and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|███████████████████████████████████████████| 15/15 [00:01<00:00, 10.24ba/s]\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:00<00:00, 12.18ba/s]\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: ID, __index_level_0__, tweet.\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 14172\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1329\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msana-ltu\u001b[0m (\u001b[33mml_ltu\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.9 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/sanala/Juputer try/afrisenti/TaskC/wandb/run-20230118_091507-pejkogyh\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtrainerfile\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ml_ltu/huggingface\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ml_ltu/huggingface/runs/pejkogyh\u001b[0m\n",
      "{'loss': 0.7427, 'learning_rate': 4.6237772761474796e-05, 'epoch': 0.23}        \n",
      "{'loss': 0.6556, 'learning_rate': 4.247554552294959e-05, 'epoch': 0.45}         \n",
      "{'loss': 0.6138, 'learning_rate': 3.8713318284424384e-05, 'epoch': 0.68}        \n",
      "{'loss': 0.575, 'learning_rate': 3.495109104589918e-05, 'epoch': 0.9}           \n",
      "{'loss': 0.4929, 'learning_rate': 3.118886380737397e-05, 'epoch': 1.13}         \n",
      " 38%|███████████████                         | 500/1329 [03:14<06:19,  2.19it/s]Saving model checkpoint to trainerfile/checkpoint-500\n",
      "Configuration saved in trainerfile/checkpoint-500/config.json\n",
      "Model weights saved in trainerfile/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in trainerfile/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in trainerfile/checkpoint-500/special_tokens_map.json\n",
      "{'loss': 0.422, 'learning_rate': 2.742663656884876e-05, 'epoch': 1.35}          \n",
      "{'loss': 0.4374, 'learning_rate': 2.3664409330323552e-05, 'epoch': 1.58}        \n",
      "{'loss': 0.43, 'learning_rate': 1.9902182091798346e-05, 'epoch': 1.81}          \n",
      "{'loss': 0.4197, 'learning_rate': 1.613995485327314e-05, 'epoch': 2.03}         \n",
      "{'loss': 0.2848, 'learning_rate': 1.2377727614747931e-05, 'epoch': 2.26}        \n",
      " 75%|█████████████████████████████▎         | 1000/1329 [06:47<02:34,  2.13it/s]Saving model checkpoint to trainerfile/checkpoint-1000\n",
      "Configuration saved in trainerfile/checkpoint-1000/config.json\n",
      "Model weights saved in trainerfile/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in trainerfile/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in trainerfile/checkpoint-1000/special_tokens_map.json\n",
      "{'loss': 0.2856, 'learning_rate': 8.615500376222724e-06, 'epoch': 2.48}         \n",
      "{'loss': 0.2857, 'learning_rate': 4.853273137697517e-06, 'epoch': 2.71}         \n",
      "{'loss': 0.2585, 'learning_rate': 1.0910458991723102e-06, 'epoch': 2.93}        \n",
      "100%|███████████████████████████████████████| 1329/1329 [09:08<00:00,  2.69it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 555.9142, 'train_samples_per_second': 76.479, 'train_steps_per_second': 2.391, 'train_loss': 0.44950420036631655, 'epoch': 3.0}\n",
      "100%|███████████████████████████████████████| 1329/1329 [09:08<00:00,  2.42it/s]\n",
      "Saving model checkpoint to setu4993/LaBSE-ha\n",
      "Configuration saved in setu4993/LaBSE-ha/config.json\n",
      "Model weights saved in setu4993/LaBSE-ha/pytorch_model.bin\n",
      "tokenizer config file saved in setu4993/LaBSE-ha/tokenizer_config.json\n",
      "Special tokens file saved in setu4993/LaBSE-ha/special_tokens_map.json\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \\ 0.473 MB of 0.473 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▂▂▃▃▄▄▅▆▆▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▂▂▃▃▄▄▅▆▆▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate █▇▇▆▆▅▅▄▃▃▂▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss █▇▆▆▄▃▄▃▃▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 3.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 1329\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.2585\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 4369738307860800.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.4495\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 555.9142\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 76.479\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 2.391\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mtrainerfile\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/ml_ltu/huggingface/runs/pejkogyh\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230118_091507-pejkogyh/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python run_train.py  --model_name  'setu4993/LaBSE'  --language 'ha' --learning_rate 5e-5 --epoch 3.0  --output_dir 'setu4993/LaBSE-ha'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-18 09:24:37.002784: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-18 09:24:37.002818: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 396\n",
      "  Batch size = 8\n",
      "100%|███████████████████████████████████████████| 50/50 [00:00<00:00, 54.13it/s]Data directory found.\n",
      "Creating submission files directory.\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 36.64it/s]\n"
     ]
    }
   ],
   "source": [
    "!python run_predict_taskC.py --model_name 'setu4993/LaBSE' --language 'or' --output_dir 'setu4993/LaBSE-ha' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-hsd",
   "language": "python",
   "name": "env-hsd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
