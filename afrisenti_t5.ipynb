{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F0je5KM7yhYs",
    "outputId": "22d40b65-b275-435f-c67c-5db687f1253b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'afriteva'...\n",
      "remote: Enumerating objects: 208, done.\u001b[K\n",
      "remote: Counting objects: 100% (208/208), done.\u001b[K\n",
      "remote: Compressing objects: 100% (129/129), done.\u001b[K\n",
      "remote: Total 208 (delta 119), reused 132 (delta 68), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (208/208), 71.04 KiB | 1.78 MiB/s, done.\n",
      "Resolving deltas: 100% (119/119), done.\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/castorini/afriteva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RdhMFR72yoCL",
    "outputId": "61c47c48-f939-47ac-f558-1ca4d1dfc6a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pytorch-lightning\n",
      "  Using cached pytorch_lightning-1.5.10-py3-none-any.whl (527 kB)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (1.19.5)\n",
      "Collecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
      "  Using cached fsspec-2022.1.0-py3-none-any.whl (133 kB)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (2.4.1)\n",
      "Collecting future>=0.17.1\n",
      "  Using cached future-0.18.2-py3-none-any.whl\n",
      "Requirement already satisfied: packaging>=17.0 in /home/sanala/.local/lib/python3.6/site-packages (from pytorch-lightning) (21.3)\n",
      "Requirement already satisfied: torch>=1.7.* in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (1.8.1)\n",
      "Collecting pyDeprecate==0.3.1\n",
      "  Using cached pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n",
      "Collecting PyYAML>=5.1\n",
      "  Using cached PyYAML-6.0-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (603 kB)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (4.49.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (3.7.4.3)\n",
      "Collecting torchmetrics>=0.4.1\n",
      "  Using cached torchmetrics-0.8.2-py3-none-any.whl (409 kB)\n",
      "Collecting setuptools==59.5.0\n",
      "  Using cached setuptools-59.5.0-py3-none-any.whl (952 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.25.1)\n",
      "Collecting aiohttp\n",
      "  Using cached aiohttp-3.8.3-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (945 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/sanala/.local/lib/python3.6/site-packages (from packaging>=17.0->pytorch-lightning) (2.4.7)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.4.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.3.3)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.14.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.15.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.36.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.11.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.0.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.8.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.32.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.25.0)\n",
      "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.7.*->pytorch-lightning) (0.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.2.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.7)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/lib/python3/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.2.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (3.4.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/lib/python3/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.22)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/lib/python3/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2018.1.18)\n",
      "Collecting charset-normalizer<3.0,>=2.0\n",
      "  Using cached charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Using cached multidict-5.2.0-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (159 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (17.4.0)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting idna-ssl>=1.0\n",
      "  Using cached idna_ssl-1.1.0-py3-none-any.whl\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Using cached frozenlist-1.2.0-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (191 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Using cached aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Collecting asynctest==0.13.0\n",
      "  Using cached asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Using cached yarl-1.7.2-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (270 kB)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (3.1.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/lib/python3/dist-packages (from rsa<5,>=3.1.4->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.4.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (3.4.0)\n",
      "Installing collected packages: setuptools, multidict, frozenlist, yarl, idna-ssl, charset-normalizer, asynctest, async-timeout, aiosignal, pyDeprecate, fsspec, aiohttp, torchmetrics, PyYAML, future, pytorch-lightning\n",
      "Successfully installed PyYAML-6.0 aiohttp-3.8.3 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 charset-normalizer-2.1.1 frozenlist-1.2.0 fsspec-2022.1.0 future-0.18.2 idna-ssl-1.1.0 multidict-5.2.0 pyDeprecate-0.3.1 pytorch-lightning-1.5.10 setuptools-59.5.0 torchmetrics-0.8.2 yarl-1.7.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "/home/sanala/anaconda3/lib/python3.8/site-packages/torchtext/_torchtext.so: undefined symbol: _ZNK3c104Type14isSubtypeOfExtERKSt10shared_ptrIS0_EPSo",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-efd8697dde72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpytorch_lightning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pytorch_lightning/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0m_PROJECT_ROOT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_PACKAGE_ROOT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCallback\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLightningDataModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLightningModule\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainer\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# See the License for the specific language governing permissions and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# limitations under the License.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_stats_monitor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDeviceStatsMonitor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mearly_stopping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpytorch_lightning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSTEP_OUTPUT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_func\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmove_data_to_device\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAllGatherGrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank_zero_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank_zero_only\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m from pytorch_lightning.utilities.enums import (  # noqa: F401\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/apply_func.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimports\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_compare_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_TORCHTEXT_LEGACY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarnings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrank_zero_deprecation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/imports.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0m_TORCH_QUANTIZE_AVAILABLE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0meg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupported_engines\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0meg\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"none\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0m_TORCHTEXT_AVAILABLE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_package_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"torchtext\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m \u001b[0m_TORCHTEXT_LEGACY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_TORCHTEXT_AVAILABLE\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_compare_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"torchtext\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"0.11.0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0m_TORCHVISION_AVAILABLE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_package_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"torchvision\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0m_XLA_AVAILABLE\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_package_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"torch_xla\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/imports.py\u001b[0m in \u001b[0;36m_compare_version\u001b[0;34m(package, op, version, use_base_version)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \"\"\"\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mpkg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mModuleNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDistributionNotFound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torchtext/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlegacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_extension\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_init_extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torchtext/vocab/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m from .vocab_factory import (\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mbuild_vocab_from_iterator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torchtext/vocab/vocab_factory.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m from torchtext._torchtext import (\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mVocab\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mVocabPybind\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m )\n",
      "\u001b[0;31mImportError\u001b[0m: /home/sanala/anaconda3/lib/python3.8/site-packages/torchtext/_torchtext.so: undefined symbol: _ZNK3c104Type14isSubtypeOfExtERKSt10shared_ptrIS0_EPSo"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torchtext\n",
      "  Downloading torchtext-0.11.2-cp36-cp36m-manylinux1_x86_64.whl (8.0 MB)\n",
      "     |████████████████████████████████| 8.0 MB 155 kB/s            \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.19.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.25.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.49.0)\n",
      "Collecting torch==1.10.2\n",
      "  Using cached torch-1.10.2-cp36-cp36m-manylinux1_x86_64.whl (881.9 MB)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==1.10.2->torchtext) (3.7.4.3)\n",
      "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch==1.10.2->torchtext) (0.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->torchtext) (2018.1.18)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->torchtext) (1.22)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/lib/python3/dist-packages (from requests->torchtext) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/lib/python3/dist-packages (from requests->torchtext) (2.6)\n",
      "Installing collected packages: torch, torchtext\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.9.1 requires torch==1.8.1, but you have torch 1.10.2 which is incompatible.\n",
      "torchaudio 0.8.1 requires torch==1.8.1, but you have torch 1.10.2 which is incompatible.\u001b[0m\n",
      "Successfully installed torch-1.10.2 torchtext-0.11.2\n"
     ]
    }
   ],
   "source": [
    "!pip install torchtext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WsRTIYDYyqrN",
    "outputId": "8d26512b-340c-4bac-fd19-e48dde9091dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://github.com/huggingface/transformers\n",
      "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-2ztxvgb6\n",
      "  Running command git clone --filter=blob:none -q https://github.com/huggingface/transformers /tmp/pip-req-build-2ztxvgb6\n",
      "  Resolved https://github.com/huggingface/transformers to commit 64b6b2b273a4c8c91fd0a9ebacffd04d404b3358\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==4.26.0.dev0) (2.25.1)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Using cached tokenizers-0.13.2.tar.gz (359 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /home/sanala/.local/lib/python3.6/site-packages (from transformers==4.26.0.dev0) (2022.9.13)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement huggingface-hub<1.0,>=0.10.0 (from transformers) (from versions: 0.0.1, 0.0.2, 0.0.3rc1, 0.0.3rc2, 0.0.5, 0.0.6, 0.0.7, 0.0.8, 0.0.9, 0.0.10, 0.0.11, 0.0.12, 0.0.13, 0.0.14, 0.0.15, 0.0.16, 0.0.17, 0.0.18, 0.0.19, 0.1.0, 0.1.1, 0.1.2, 0.2.0, 0.2.1, 0.4.0)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for huggingface-hub<1.0,>=0.10.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w__FTW6Myt0s",
    "outputId": "783c87da-c73d-4c4f-994e-17302279b496"
   },
   "outputs": [],
   "source": [
    "!pip install flax jax torch datasets sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_dbkdl8o3nRs",
    "outputId": "a40582ac-0a38-4e67-bfbd-00ce7f3c0d90"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pandas==1.2.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "sa8JcdcXy3AK"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Mp2-Duyy6oB",
    "outputId": "0008df36-dc21-4124-db8f-012d97cb98ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'afrisent-semeval-2023'...\n",
      "remote: Enumerating objects: 550, done.\u001b[K\n",
      "remote: Counting objects: 100% (205/205), done.\u001b[K\n",
      "remote: Compressing objects: 100% (86/86), done.\u001b[K\n",
      "remote: Total 550 (delta 120), reused 197 (delta 118), pack-reused 345\u001b[K\n",
      "Receiving objects: 100% (550/550), 17.30 MiB | 11.36 MiB/s, done.\n",
      "Resolving deltas: 100% (257/257), done.\n"
     ]
    }
   ],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CqY0OqWRymzg",
    "outputId": "e2e1348b-48ec-443b-a08a-d5c79cb14789"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sanala/Juputer try/afrisenti\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a1_V-gqPiZug"
   },
   "source": [
    "# AfriTeVa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iiWt8vdQywI0",
    "outputId": "bcd572bd-e5a1-4b71-8142-3e16ace137e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"afriteva/classification_scripts/classification_trainer.py\", line 2, in <module>\n",
      "    from utils import LoggingCallback\n",
      "  File \"/home/sanala/Juputer try/afrisenti/afriteva/classification_scripts/utils.py\", line 6, in <module>\n",
      "    import pytorch_lightning as pl\n",
      "  File \"/home/sanala/anaconda3/lib/python3.8/site-packages/pytorch_lightning/__init__.py\", line 20, in <module>\n",
      "    from pytorch_lightning.callbacks import Callback  # noqa: E402\n",
      "  File \"/home/sanala/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/__init__.py\", line 14, in <module>\n",
      "    from pytorch_lightning.callbacks.base import Callback\n",
      "  File \"/home/sanala/anaconda3/lib/python3.8/site-packages/pytorch_lightning/callbacks/base.py\", line 26, in <module>\n",
      "    from pytorch_lightning.utilities.types import STEP_OUTPUT\n",
      "  File \"/home/sanala/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/__init__.py\", line 18, in <module>\n",
      "    from pytorch_lightning.utilities.apply_func import move_data_to_device  # noqa: F401\n",
      "  File \"/home/sanala/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/apply_func.py\", line 26, in <module>\n",
      "    from pytorch_lightning.utilities.imports import _compare_version, _TORCHTEXT_LEGACY\n",
      "  File \"/home/sanala/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/imports.py\", line 121, in <module>\n",
      "    _TORCHTEXT_LEGACY: bool = _TORCHTEXT_AVAILABLE and _compare_version(\"torchtext\", operator.lt, \"0.11.0\")\n",
      "  File \"/home/sanala/anaconda3/lib/python3.8/site-packages/pytorch_lightning/utilities/imports.py\", line 77, in _compare_version\n",
      "    pkg = importlib.import_module(package)\n",
      "  File \"/home/sanala/anaconda3/lib/python3.8/importlib/__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/home/sanala/anaconda3/lib/python3.8/site-packages/torchtext/__init__.py\", line 5, in <module>\n",
      "    from . import vocab\n",
      "  File \"/home/sanala/anaconda3/lib/python3.8/site-packages/torchtext/vocab/__init__.py\", line 11, in <module>\n",
      "    from .vocab_factory import (\n",
      "  File \"/home/sanala/anaconda3/lib/python3.8/site-packages/torchtext/vocab/vocab_factory.py\", line 4, in <module>\n",
      "    from torchtext._torchtext import (\n",
      "ImportError: /home/sanala/anaconda3/lib/python3.8/site-packages/torchtext/_torchtext.so: undefined symbol: _ZNK3c104Type14isSubtypeOfExtERKSt10shared_ptrIS0_EPSo\n"
     ]
    }
   ],
   "source": [
    "train_data_path=\"/home/sanala/Juputer try/afrisenti/TaskB/multilingual_train.tsv\"\n",
    "model_name_or_path=\"castorini/afriteva_small\"\n",
    "tokenizer_name_or_path=\"castorini/afriteva_small\"\n",
    "max_seq_length=\"128\"\n",
    "output_dir=\"/content/afriteva_small\"\n",
    "learning_rate=\"3e-4\"\n",
    "train_batch_size=\"16\"\n",
    "eval_batch_size=\"16\"\n",
    "num_train_epochs=\"5\"\n",
    "gradient_accumulation_steps=\"16\"\n",
    "class_labels=\"positive,negative,neutral\"\n",
    "data_column=\"tweet\"\n",
    "target_column=\"label\"\n",
    "prompt=\"classify: \"\n",
    "\n",
    "\n",
    "!python afriteva/classification_scripts/classification_trainer.py --train_data_path=$train_data_path \\\n",
    "    --model_name_or_path=$model_name_or_path \\\n",
    "    --tokenizer_name_or_path=$tokenizer_name_or_path \\\n",
    "    --output_dir=$output_dir \\\n",
    "    --max_seq_length=$max_seq_length \\\n",
    "    --train_batch_size=$train_batch_size \\\n",
    "    --eval_batch_size=$eval_batch_size \\\n",
    "    --num_train_epochs=$num_train_epochs \\\n",
    "    --gradient_accumulation_steps=$gradient_accumulation_steps \\\n",
    "    --class_labels=$class_labels \\\n",
    "    --target_column=$target_column \\\n",
    "    --data_column=$data_column \\\n",
    "    --prompt=$prompt \\\n",
    "    --learning_rate=\"3e-4\" \\\n",
    "    --weight_decay=\"0.0\" \\\n",
    "    --adam_epsilon=\"1e-8\" \\\n",
    "    --warmup_steps=\"0\" \\\n",
    "    --n_gpu=\"1\" \\\n",
    "    --fp_16=\"false\" \\\n",
    "    --max_grad_norm=\"1.0\" \\\n",
    "    --opt_level=\"O1\" \\\n",
    "    --seed=\"42\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kn7ubdiEiWcH"
   },
   "source": [
    "#AfriMT5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GrSWdrS0TqOx",
    "outputId": "87b46ec7-655e-4700-c1ce-5f6ff602efc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Arguments Namespace(adam_epsilon=1e-08, class_labels='positive,negative,neutral', data_column='tweet', early_stop_callback=False, eval_batch_size=4, eval_data_path='/content/afrisent-semeval-2023/multilingual-training/SubtaskA/yo_dev_new.tsv', fp_16=True, gradient_accumulation_steps=16, lang='yoruba', learning_rate=0.0003, max_grad_norm=1.0, max_seq_length=128, model_name_or_path='masakhane/afri-mt5-base', n_gpu=1, num_train_epochs=5, opt_level='O1', output_dir='/content/afrimt5_small', prompt='classify:', seed=42, target_column='label', test_data_path='/content/afrisent-semeval-2023/multilingual-training/SubtaskA/yo_dev_new.tsv', tokenizer_name_or_path='masakhane/afri-mt5-base', train_batch_size=4, train_data_path='/content/afrisent-semeval-2023/multilingual-training/SubtaskA/yo_train_new.tsv', use_fast_tokenizer=True, warmup_steps=0.0, weight_decay=0.0)\n",
      "[INFO] Building dataset...\n",
      "[INFO] Done Building the Dataset\n",
      "[INFO] Building dataset...\n",
      "[INFO] Done Building the Dataset\n",
      "[INFO] Building dataset...\n",
      "[INFO] Done Building the Dataset\n",
      "[INFO] Training model .........\n",
      "You are using a model of type mt5 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/configuration_validator.py:92: UserWarning: When using `Trainer(accumulate_grad_batches != 1)` and overriding `LightningModule.optimizer_{step,zero_grad}`, the hooks will not be called on every batch (rather, they are called on every optimization step).\n",
      "  rank_zero_warn(\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 582 M \n",
      "-----------------------------------------------------\n",
      "582 M     Trainable params\n",
      "0         Non-trainable params\n",
      "582 M     Total params\n",
      "2,329.605 Total estimated model params size (MB)\n",
      "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/loggers/csv_logs.py:57: UserWarning: Experiment logs directory /content/afrimt5_small/ exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "  rank_zero_warn(\n",
      "Sanity Checking: 0it [00:00, ?it/s]/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "Epoch 0:  27% 585/2132 [01:54<05:03,  5.09it/s, loss=1.29, v_num=]/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "train_data_path=\"/content/afrisent-semeval-2023/multilingual-training/SubtaskA/yo_train_new.tsv\"\n",
    "eval_data_path=\"/content/afrisent-semeval-2023/multilingual-training/SubtaskA/yo_dev_new.tsv\"\n",
    "test_data_path=\"/content/afrisent-semeval-2023/multilingual-training/SubtaskA/yo_dev_new.tsv\"\n",
    "model_name_or_path=\"masakhane/afri-mt5-base\"\n",
    "tokenizer_name_or_path=\"masakhane/afri-mt5-base\"\n",
    "max_seq_length=\"128\"\n",
    "output_dir=\"/content/afrimt5_small\"\n",
    "learning_rate=\"3e-4\"\n",
    "train_batch_size=\"4\"\n",
    "eval_batch_size=\"4\"\n",
    "num_train_epochs=\"5\"\n",
    "gradient_accumulation_steps=\"16\"\n",
    "lang=\"yoruba\"\n",
    "class_labels=\"positive,negative,neutral\"\n",
    "data_column=\"tweet\"\n",
    "target_column=\"label\"\n",
    "prompt=\"classify: \"\n",
    "\n",
    "\n",
    "!python3 /content/afriteva/classification_scripts/classification_trainer.py --train_data_path=$train_data_path \\\n",
    "    --eval_data_path=$eval_data_path \\\n",
    "    --test_data_path=$test_data_path \\\n",
    "    --model_name_or_path=$model_name_or_path \\\n",
    "    --tokenizer_name_or_path=$tokenizer_name_or_path \\\n",
    "    --output_dir=$output_dir \\\n",
    "    --max_seq_length=$max_seq_length \\\n",
    "    --train_batch_size=$train_batch_size \\\n",
    "    --eval_batch_size=$eval_batch_size \\\n",
    "    --num_train_epochs=$num_train_epochs \\\n",
    "    --gradient_accumulation_steps=$gradient_accumulation_steps \\\n",
    "    --lang=$lang \\\n",
    "    --class_labels=$class_labels \\\n",
    "    --target_column=$target_column \\\n",
    "    --data_column=$data_column \\\n",
    "    --prompt=$prompt \\\n",
    "    --learning_rate=\"3e-4\" \\\n",
    "    --weight_decay=\"0.0\" \\\n",
    "    --adam_epsilon=\"1e-8\" \\\n",
    "    --warmup_steps=\"0\" \\\n",
    "    --n_gpu=\"1\" \\\n",
    "    --fp_16=\"false\" \\\n",
    "    --max_grad_norm=\"1.0\" \\\n",
    "    --opt_level=\"O1\" \\\n",
    "    --seed=\"42\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-JUe7kln1w3"
   },
   "source": [
    "# AfriByT5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ADJ8iitUSFd",
    "outputId": "e73dc6a5-32e5-4c0b-fedf-4e734cb40539"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Arguments Namespace(adam_epsilon=1e-08, class_labels='positive,negative,neutral', data_column='tweet', early_stop_callback=False, eval_batch_size=4, eval_data_path='/content/afrisent-semeval-2023/multilingual-training/SubtaskA/yo_dev_new.tsv', fp_16=True, gradient_accumulation_steps=16, lang='yoruba', learning_rate=0.0003, max_grad_norm=1.0, max_seq_length=128, model_name_or_path='masakhane/afri-byt5-base', n_gpu=1, num_train_epochs=5, opt_level='O1', output_dir='/content/afrimt5_small', prompt='classify:', seed=42, target_column='label', test_data_path='/content/afrisent-semeval-2023/multilingual-training/SubtaskA/yo_dev_new.tsv', tokenizer_name_or_path='masakhane/afri-byt5-base', train_batch_size=4, train_data_path='/content/afrisent-semeval-2023/multilingual-training/SubtaskA/yo_train_new.tsv', use_fast_tokenizer=True, warmup_steps=0.0, weight_decay=0.0)\n",
      "[INFO] Building dataset...\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/models/byt5/tokenization_byt5.py:149: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  warnings.warn(\n",
      "[INFO] Done Building the Dataset\n",
      "[INFO] Building dataset...\n",
      "[INFO] Done Building the Dataset\n",
      "[INFO] Building dataset...\n",
      "[INFO] Done Building the Dataset\n",
      "[INFO] Training model .........\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/configuration_validator.py:92: UserWarning: When using `Trainer(accumulate_grad_batches != 1)` and overriding `LightningModule.optimizer_{step,zero_grad}`, the hooks will not be called on every batch (rather, they are called on every optimization step).\n",
      "  rank_zero_warn(\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 581 M \n",
      "-----------------------------------------------------\n",
      "581 M     Trainable params\n",
      "0         Non-trainable params\n",
      "581 M     Total params\n",
      "2,326.613 Total estimated model params size (MB)\n",
      "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/loggers/csv_logs.py:57: UserWarning: Experiment logs directory /content/afrimt5_small/ exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "  rank_zero_warn(\n",
      "Sanity Checking: 0it [00:00, ?it/s]/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "Epoch 0:  80% 1705/2132 [17:49<04:27,  1.59it/s, loss=0.262, v_num=]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0% 0/427 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0% 0/427 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  80% 1706/2132 [17:50<04:27,  1.59it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  80% 1707/2132 [17:50<04:26,  1.59it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  80% 1708/2132 [17:50<04:25,  1.60it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  80% 1709/2132 [17:50<04:25,  1.60it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  80% 1710/2132 [17:51<04:24,  1.60it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  80% 1711/2132 [17:51<04:23,  1.60it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  80% 1712/2132 [17:51<04:22,  1.60it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  80% 1713/2132 [17:51<04:22,  1.60it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  80% 1714/2132 [17:51<04:21,  1.60it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  80% 1715/2132 [17:51<04:20,  1.60it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  80% 1716/2132 [17:51<04:19,  1.60it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  81% 1717/2132 [17:52<04:19,  1.60it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  81% 1718/2132 [17:52<04:18,  1.60it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  81% 1719/2132 [17:52<04:17,  1.60it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  81% 1720/2132 [17:52<04:16,  1.60it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  81% 1721/2132 [17:52<04:16,  1.60it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  81% 1722/2132 [17:52<04:15,  1.61it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  81% 1723/2132 [17:52<04:14,  1.61it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  81% 1724/2132 [17:53<04:13,  1.61it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  81% 1725/2132 [17:53<04:13,  1.61it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  81% 1726/2132 [17:53<04:12,  1.61it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  81% 1727/2132 [17:53<04:11,  1.61it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  81% 1728/2132 [17:53<04:11,  1.61it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  81% 1729/2132 [17:53<04:10,  1.61it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  81% 1730/2132 [17:54<04:09,  1.61it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  81% 1731/2132 [17:54<04:08,  1.61it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  81% 1732/2132 [17:54<04:08,  1.61it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  81% 1733/2132 [17:54<04:07,  1.61it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  81% 1734/2132 [17:54<04:06,  1.61it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  81% 1735/2132 [17:54<04:05,  1.61it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  81% 1736/2132 [17:54<04:05,  1.62it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  81% 1737/2132 [17:55<04:04,  1.62it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  82% 1738/2132 [17:55<04:03,  1.62it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  82% 1739/2132 [17:55<04:03,  1.62it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  82% 1740/2132 [17:55<04:02,  1.62it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  82% 1741/2132 [17:55<04:01,  1.62it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  82% 1742/2132 [17:55<04:00,  1.62it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  82% 1743/2132 [17:55<04:00,  1.62it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  82% 1744/2132 [17:56<03:59,  1.62it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  82% 1745/2132 [17:56<03:58,  1.62it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  82% 1746/2132 [17:56<03:57,  1.62it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  82% 1747/2132 [17:56<03:57,  1.62it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  82% 1748/2132 [17:56<03:56,  1.62it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  82% 1749/2132 [17:56<03:55,  1.62it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  82% 1750/2132 [17:56<03:55,  1.62it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  82% 1751/2132 [17:57<03:54,  1.63it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  82% 1752/2132 [17:57<03:53,  1.63it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  82% 1753/2132 [17:57<03:52,  1.63it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  82% 1754/2132 [17:57<03:52,  1.63it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  82% 1755/2132 [17:57<03:51,  1.63it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  82% 1756/2132 [17:57<03:50,  1.63it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  82% 1757/2132 [17:58<03:50,  1.63it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  82% 1758/2132 [17:58<03:49,  1.63it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  83% 1759/2132 [17:58<03:48,  1.63it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  83% 1760/2132 [17:58<03:47,  1.63it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  83% 1761/2132 [17:58<03:47,  1.63it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  83% 1762/2132 [17:58<03:46,  1.63it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  83% 1763/2132 [17:58<03:45,  1.63it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  83% 1764/2132 [17:59<03:45,  1.63it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  83% 1765/2132 [17:59<03:44,  1.64it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  83% 1766/2132 [17:59<03:43,  1.64it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  83% 1767/2132 [17:59<03:42,  1.64it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  83% 1768/2132 [17:59<03:42,  1.64it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  83% 1769/2132 [17:59<03:41,  1.64it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  83% 1770/2132 [17:59<03:40,  1.64it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  83% 1771/2132 [18:00<03:40,  1.64it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  83% 1772/2132 [18:00<03:39,  1.64it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  83% 1773/2132 [18:00<03:38,  1.64it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  83% 1774/2132 [18:00<03:38,  1.64it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  83% 1775/2132 [18:00<03:37,  1.64it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  83% 1776/2132 [18:00<03:36,  1.64it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  83% 1777/2132 [18:01<03:35,  1.64it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  83% 1778/2132 [18:01<03:35,  1.64it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  83% 1779/2132 [18:01<03:34,  1.65it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  83% 1780/2132 [18:01<03:33,  1.65it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  84% 1781/2132 [18:01<03:33,  1.65it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  84% 1782/2132 [18:01<03:32,  1.65it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  84% 1783/2132 [18:01<03:31,  1.65it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  84% 1784/2132 [18:02<03:31,  1.65it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  84% 1785/2132 [18:02<03:30,  1.65it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  84% 1786/2132 [18:02<03:29,  1.65it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  84% 1787/2132 [18:02<03:28,  1.65it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  84% 1788/2132 [18:02<03:28,  1.65it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  84% 1789/2132 [18:02<03:27,  1.65it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  84% 1790/2132 [18:02<03:26,  1.65it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  84% 1791/2132 [18:03<03:26,  1.65it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  84% 1792/2132 [18:03<03:25,  1.65it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  84% 1793/2132 [18:03<03:24,  1.65it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  84% 1794/2132 [18:03<03:24,  1.66it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  84% 1795/2132 [18:03<03:23,  1.66it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  84% 1796/2132 [18:03<03:22,  1.66it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  84% 1797/2132 [18:04<03:22,  1.66it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  84% 1798/2132 [18:04<03:21,  1.66it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  84% 1799/2132 [18:04<03:20,  1.66it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  84% 1800/2132 [18:04<03:20,  1.66it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  84% 1801/2132 [18:04<03:19,  1.66it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  85% 1802/2132 [18:04<03:18,  1.66it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  85% 1803/2132 [18:04<03:17,  1.66it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  85% 1804/2132 [18:05<03:17,  1.66it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  85% 1805/2132 [18:05<03:16,  1.66it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  85% 1806/2132 [18:05<03:15,  1.66it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  85% 1807/2132 [18:05<03:15,  1.66it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  85% 1808/2132 [18:05<03:14,  1.67it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  85% 1809/2132 [18:05<03:13,  1.67it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  85% 1810/2132 [18:06<03:13,  1.67it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  85% 1811/2132 [18:06<03:12,  1.67it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  85% 1812/2132 [18:06<03:11,  1.67it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  85% 1813/2132 [18:06<03:11,  1.67it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  85% 1814/2132 [18:06<03:10,  1.67it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  85% 1815/2132 [18:06<03:09,  1.67it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  85% 1816/2132 [18:06<03:09,  1.67it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  85% 1817/2132 [18:07<03:08,  1.67it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  85% 1818/2132 [18:07<03:07,  1.67it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  85% 1819/2132 [18:07<03:07,  1.67it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  85% 1820/2132 [18:07<03:06,  1.67it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  85% 1821/2132 [18:07<03:05,  1.67it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  85% 1822/2132 [18:07<03:05,  1.67it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  86% 1823/2132 [18:08<03:04,  1.68it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  86% 1824/2132 [18:08<03:03,  1.68it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  86% 1825/2132 [18:08<03:03,  1.68it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  86% 1826/2132 [18:08<03:02,  1.68it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  86% 1827/2132 [18:08<03:01,  1.68it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  86% 1828/2132 [18:08<03:01,  1.68it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  86% 1829/2132 [18:08<03:00,  1.68it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  86% 1830/2132 [18:09<02:59,  1.68it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  86% 1831/2132 [18:09<02:59,  1.68it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  86% 1832/2132 [18:09<02:58,  1.68it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  86% 1833/2132 [18:09<02:57,  1.68it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  86% 1834/2132 [18:09<02:57,  1.68it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  86% 1835/2132 [18:09<02:56,  1.68it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  86% 1836/2132 [18:10<02:55,  1.68it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  86% 1837/2132 [18:10<02:55,  1.69it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  86% 1838/2132 [18:10<02:54,  1.69it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  86% 1839/2132 [18:10<02:53,  1.69it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  86% 1840/2132 [18:10<02:53,  1.69it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  86% 1841/2132 [18:10<02:52,  1.69it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  86% 1842/2132 [18:10<02:51,  1.69it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  86% 1843/2132 [18:11<02:51,  1.69it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  86% 1844/2132 [18:11<02:50,  1.69it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  87% 1845/2132 [18:11<02:49,  1.69it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  87% 1846/2132 [18:11<02:49,  1.69it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  87% 1847/2132 [18:11<02:48,  1.69it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  87% 1848/2132 [18:11<02:47,  1.69it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  87% 1849/2132 [18:12<02:47,  1.69it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  87% 1850/2132 [18:12<02:46,  1.69it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  87% 1851/2132 [18:12<02:45,  1.69it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  87% 1852/2132 [18:12<02:45,  1.70it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  87% 1853/2132 [18:12<02:44,  1.70it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  87% 1854/2132 [18:12<02:43,  1.70it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  87% 1855/2132 [18:12<02:43,  1.70it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  87% 1856/2132 [18:13<02:42,  1.70it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  87% 1857/2132 [18:13<02:41,  1.70it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  87% 1858/2132 [18:13<02:41,  1.70it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  87% 1859/2132 [18:13<02:40,  1.70it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  87% 1860/2132 [18:13<02:39,  1.70it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  87% 1861/2132 [18:13<02:39,  1.70it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  87% 1862/2132 [18:14<02:38,  1.70it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  87% 1863/2132 [18:14<02:37,  1.70it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  87% 1864/2132 [18:14<02:37,  1.70it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  87% 1865/2132 [18:14<02:36,  1.70it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  88% 1866/2132 [18:14<02:36,  1.70it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  88% 1867/2132 [18:14<02:35,  1.71it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  88% 1868/2132 [18:14<02:34,  1.71it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  88% 1869/2132 [18:15<02:34,  1.71it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  88% 1870/2132 [18:15<02:33,  1.71it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  88% 1871/2132 [18:15<02:32,  1.71it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  88% 1872/2132 [18:15<02:32,  1.71it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  88% 1873/2132 [18:15<02:31,  1.71it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  88% 1874/2132 [18:15<02:30,  1.71it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  88% 1875/2132 [18:16<02:30,  1.71it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  88% 1876/2132 [18:16<02:29,  1.71it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  88% 1877/2132 [18:16<02:28,  1.71it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  88% 1878/2132 [18:16<02:28,  1.71it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  88% 1879/2132 [18:16<02:27,  1.71it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  88% 1880/2132 [18:16<02:27,  1.71it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  88% 1881/2132 [18:16<02:26,  1.71it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  88% 1882/2132 [18:17<02:25,  1.72it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  88% 1883/2132 [18:17<02:25,  1.72it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  88% 1884/2132 [18:17<02:24,  1.72it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  88% 1885/2132 [18:17<02:23,  1.72it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  88% 1886/2132 [18:17<02:23,  1.72it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  89% 1887/2132 [18:17<02:22,  1.72it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  89% 1888/2132 [18:18<02:21,  1.72it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  89% 1889/2132 [18:18<02:21,  1.72it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  89% 1890/2132 [18:18<02:20,  1.72it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  89% 1891/2132 [18:18<02:20,  1.72it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  89% 1892/2132 [18:18<02:19,  1.72it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  89% 1893/2132 [18:18<02:18,  1.72it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  89% 1894/2132 [18:19<02:18,  1.72it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  89% 1895/2132 [18:19<02:17,  1.72it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  89% 1896/2132 [18:19<02:16,  1.72it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  89% 1897/2132 [18:19<02:16,  1.73it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  89% 1898/2132 [18:19<02:15,  1.73it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  89% 1899/2132 [18:19<02:14,  1.73it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  89% 1900/2132 [18:19<02:14,  1.73it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  89% 1901/2132 [18:20<02:13,  1.73it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  89% 1902/2132 [18:20<02:13,  1.73it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  89% 1903/2132 [18:20<02:12,  1.73it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  89% 1904/2132 [18:20<02:11,  1.73it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  89% 1905/2132 [18:20<02:11,  1.73it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  89% 1906/2132 [18:20<02:10,  1.73it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  89% 1907/2132 [18:21<02:09,  1.73it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  89% 1908/2132 [18:21<02:09,  1.73it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  90% 1909/2132 [18:21<02:08,  1.73it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  90% 1910/2132 [18:21<02:08,  1.73it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  90% 1911/2132 [18:21<02:07,  1.73it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  90% 1912/2132 [18:21<02:06,  1.74it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  90% 1913/2132 [18:22<02:06,  1.74it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  90% 1914/2132 [18:22<02:05,  1.74it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  90% 1915/2132 [18:22<02:04,  1.74it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  90% 1916/2132 [18:22<02:04,  1.74it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  90% 1917/2132 [18:22<02:03,  1.74it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  90% 1918/2132 [18:22<02:03,  1.74it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  90% 1919/2132 [18:22<02:02,  1.74it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  90% 1920/2132 [18:23<02:01,  1.74it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  90% 1921/2132 [18:23<02:01,  1.74it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  90% 1922/2132 [18:23<02:00,  1.74it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  90% 1923/2132 [18:23<01:59,  1.74it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  90% 1924/2132 [18:23<01:59,  1.74it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  90% 1925/2132 [18:23<01:58,  1.74it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  90% 1926/2132 [18:24<01:58,  1.74it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  90% 1927/2132 [18:24<01:57,  1.75it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  90% 1928/2132 [18:24<01:56,  1.75it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  90% 1929/2132 [18:24<01:56,  1.75it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  91% 1930/2132 [18:24<01:55,  1.75it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  91% 1931/2132 [18:24<01:55,  1.75it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  91% 1932/2132 [18:25<01:54,  1.75it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  91% 1933/2132 [18:25<01:53,  1.75it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  91% 1934/2132 [18:25<01:53,  1.75it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  91% 1935/2132 [18:25<01:52,  1.75it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  91% 1936/2132 [18:25<01:51,  1.75it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  91% 1937/2132 [18:25<01:51,  1.75it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  91% 1938/2132 [18:25<01:50,  1.75it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  91% 1939/2132 [18:26<01:50,  1.75it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  91% 1940/2132 [18:26<01:49,  1.75it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  91% 1941/2132 [18:26<01:48,  1.75it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  91% 1942/2132 [18:26<01:48,  1.75it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  91% 1943/2132 [18:26<01:47,  1.76it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  91% 1944/2132 [18:26<01:47,  1.76it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  91% 1945/2132 [18:27<01:46,  1.76it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  91% 1946/2132 [18:27<01:45,  1.76it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  91% 1947/2132 [18:27<01:45,  1.76it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  91% 1948/2132 [18:27<01:44,  1.76it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  91% 1949/2132 [18:27<01:44,  1.76it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  91% 1950/2132 [18:27<01:43,  1.76it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  92% 1951/2132 [18:28<01:42,  1.76it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  92% 1952/2132 [18:28<01:42,  1.76it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  92% 1953/2132 [18:28<01:41,  1.76it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  92% 1954/2132 [18:28<01:40,  1.76it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  92% 1955/2132 [18:28<01:40,  1.76it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  92% 1956/2132 [18:28<01:39,  1.76it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  92% 1957/2132 [18:29<01:39,  1.76it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  92% 1958/2132 [18:29<01:38,  1.77it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  92% 1959/2132 [18:29<01:37,  1.77it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  92% 1960/2132 [18:29<01:37,  1.77it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  92% 1961/2132 [18:29<01:36,  1.77it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  92% 1962/2132 [18:29<01:36,  1.77it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  92% 1963/2132 [18:30<01:35,  1.77it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  92% 1964/2132 [18:30<01:34,  1.77it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  92% 1965/2132 [18:30<01:34,  1.77it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  92% 1966/2132 [18:30<01:33,  1.77it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  92% 1967/2132 [18:30<01:33,  1.77it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  92% 1968/2132 [18:30<01:32,  1.77it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  92% 1969/2132 [18:30<01:31,  1.77it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  92% 1970/2132 [18:31<01:31,  1.77it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  92% 1971/2132 [18:31<01:30,  1.77it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  92% 1972/2132 [18:31<01:30,  1.77it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  93% 1973/2132 [18:31<01:29,  1.77it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  93% 1974/2132 [18:31<01:28,  1.78it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  93% 1975/2132 [18:31<01:28,  1.78it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  93% 1976/2132 [18:32<01:27,  1.78it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  93% 1977/2132 [18:32<01:27,  1.78it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  93% 1978/2132 [18:32<01:26,  1.78it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  93% 1979/2132 [18:32<01:26,  1.78it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  93% 1980/2132 [18:32<01:25,  1.78it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  93% 1981/2132 [18:32<01:24,  1.78it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  93% 1982/2132 [18:33<01:24,  1.78it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  93% 1983/2132 [18:33<01:23,  1.78it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  93% 1984/2132 [18:33<01:23,  1.78it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  93% 1985/2132 [18:33<01:22,  1.78it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  93% 1986/2132 [18:33<01:21,  1.78it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  93% 1987/2132 [18:33<01:21,  1.78it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  93% 1988/2132 [18:34<01:20,  1.78it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  93% 1989/2132 [18:34<01:20,  1.79it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  93% 1990/2132 [18:34<01:19,  1.79it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  93% 1991/2132 [18:34<01:18,  1.79it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  93% 1992/2132 [18:34<01:18,  1.79it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  93% 1993/2132 [18:34<01:17,  1.79it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  94% 1994/2132 [18:35<01:17,  1.79it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  94% 1995/2132 [18:35<01:16,  1.79it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  94% 1996/2132 [18:35<01:15,  1.79it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  94% 1997/2132 [18:35<01:15,  1.79it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  94% 1998/2132 [18:35<01:14,  1.79it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  94% 1999/2132 [18:35<01:14,  1.79it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  94% 2000/2132 [18:36<01:13,  1.79it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  94% 2001/2132 [18:36<01:13,  1.79it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  94% 2002/2132 [18:36<01:12,  1.79it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  94% 2003/2132 [18:36<01:11,  1.79it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  94% 2004/2132 [18:36<01:11,  1.79it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  94% 2005/2132 [18:36<01:10,  1.80it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  94% 2006/2132 [18:36<01:10,  1.80it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  94% 2007/2132 [18:37<01:09,  1.80it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  94% 2008/2132 [18:37<01:08,  1.80it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  94% 2009/2132 [18:37<01:08,  1.80it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  94% 2010/2132 [18:37<01:07,  1.80it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  94% 2011/2132 [18:37<01:07,  1.80it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  94% 2012/2132 [18:37<01:06,  1.80it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  94% 2013/2132 [18:38<01:06,  1.80it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  94% 2014/2132 [18:38<01:05,  1.80it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  95% 2015/2132 [18:38<01:04,  1.80it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  95% 2016/2132 [18:38<01:04,  1.80it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  95% 2017/2132 [18:38<01:03,  1.80it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  95% 2018/2132 [18:38<01:03,  1.80it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  95% 2019/2132 [18:39<01:02,  1.80it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  95% 2020/2132 [18:39<01:02,  1.80it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  95% 2021/2132 [18:39<01:01,  1.81it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  95% 2022/2132 [18:39<01:00,  1.81it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  95% 2023/2132 [18:39<01:00,  1.81it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  95% 2024/2132 [18:39<00:59,  1.81it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  95% 2025/2132 [18:40<00:59,  1.81it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  95% 2026/2132 [18:40<00:58,  1.81it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  95% 2027/2132 [18:40<00:58,  1.81it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  95% 2028/2132 [18:40<00:57,  1.81it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  95% 2029/2132 [18:40<00:56,  1.81it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  95% 2030/2132 [18:40<00:56,  1.81it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  95% 2031/2132 [18:41<00:55,  1.81it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  95% 2032/2132 [18:41<00:55,  1.81it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  95% 2033/2132 [18:41<00:54,  1.81it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  95% 2034/2132 [18:41<00:54,  1.81it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  95% 2035/2132 [18:41<00:53,  1.81it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  95% 2036/2132 [18:41<00:52,  1.81it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  96% 2037/2132 [18:42<00:52,  1.82it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  96% 2038/2132 [18:42<00:51,  1.82it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  96% 2039/2132 [18:42<00:51,  1.82it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  96% 2040/2132 [18:42<00:50,  1.82it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  96% 2041/2132 [18:42<00:50,  1.82it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  96% 2042/2132 [18:42<00:49,  1.82it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  96% 2043/2132 [18:42<00:48,  1.82it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  96% 2044/2132 [18:43<00:48,  1.82it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  96% 2045/2132 [18:43<00:47,  1.82it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  96% 2046/2132 [18:43<00:47,  1.82it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  96% 2047/2132 [18:43<00:46,  1.82it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  96% 2048/2132 [18:43<00:46,  1.82it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  96% 2049/2132 [18:43<00:45,  1.82it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  96% 2050/2132 [18:44<00:44,  1.82it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  96% 2051/2132 [18:44<00:44,  1.82it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  96% 2052/2132 [18:44<00:43,  1.82it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  96% 2053/2132 [18:44<00:43,  1.83it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  96% 2054/2132 [18:44<00:42,  1.83it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  96% 2055/2132 [18:44<00:42,  1.83it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  96% 2056/2132 [18:45<00:41,  1.83it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  96% 2057/2132 [18:45<00:41,  1.83it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  97% 2058/2132 [18:45<00:40,  1.83it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  97% 2059/2132 [18:45<00:39,  1.83it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  97% 2060/2132 [18:45<00:39,  1.83it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  97% 2061/2132 [18:45<00:38,  1.83it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  97% 2062/2132 [18:45<00:38,  1.83it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  97% 2063/2132 [18:46<00:37,  1.83it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  97% 2064/2132 [18:46<00:37,  1.83it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  97% 2065/2132 [18:46<00:36,  1.83it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  97% 2066/2132 [18:46<00:35,  1.83it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  97% 2067/2132 [18:46<00:35,  1.83it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  97% 2068/2132 [18:46<00:34,  1.84it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  97% 2069/2132 [18:47<00:34,  1.84it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  97% 2070/2132 [18:47<00:33,  1.84it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  97% 2071/2132 [18:47<00:33,  1.84it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  97% 2072/2132 [18:47<00:32,  1.84it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  97% 2073/2132 [18:47<00:32,  1.84it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  97% 2074/2132 [18:47<00:31,  1.84it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  97% 2075/2132 [18:48<00:30,  1.84it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  97% 2076/2132 [18:48<00:30,  1.84it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  97% 2077/2132 [18:48<00:29,  1.84it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  97% 2078/2132 [18:48<00:29,  1.84it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  98% 2079/2132 [18:48<00:28,  1.84it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  98% 2080/2132 [18:48<00:28,  1.84it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  98% 2081/2132 [18:48<00:27,  1.84it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  98% 2082/2132 [18:49<00:27,  1.84it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  98% 2083/2132 [18:49<00:26,  1.84it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  98% 2084/2132 [18:49<00:26,  1.85it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  98% 2085/2132 [18:49<00:25,  1.85it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  98% 2086/2132 [18:49<00:24,  1.85it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  98% 2087/2132 [18:49<00:24,  1.85it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  98% 2088/2132 [18:50<00:23,  1.85it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  98% 2089/2132 [18:50<00:23,  1.85it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  98% 2090/2132 [18:50<00:22,  1.85it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  98% 2091/2132 [18:50<00:22,  1.85it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  98% 2092/2132 [18:50<00:21,  1.85it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  98% 2093/2132 [18:50<00:21,  1.85it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  98% 2094/2132 [18:51<00:20,  1.85it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  98% 2095/2132 [18:51<00:19,  1.85it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  98% 2096/2132 [18:51<00:19,  1.85it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  98% 2097/2132 [18:51<00:18,  1.85it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  98% 2098/2132 [18:51<00:18,  1.85it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  98% 2099/2132 [18:51<00:17,  1.85it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  98% 2100/2132 [18:51<00:17,  1.86it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  99% 2101/2132 [18:52<00:16,  1.86it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  99% 2102/2132 [18:52<00:16,  1.86it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  99% 2103/2132 [18:52<00:15,  1.86it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  99% 2104/2132 [18:52<00:15,  1.86it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  99% 2105/2132 [18:52<00:14,  1.86it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  99% 2106/2132 [18:52<00:13,  1.86it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  99% 2107/2132 [18:53<00:13,  1.86it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  99% 2108/2132 [18:53<00:12,  1.86it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  99% 2109/2132 [18:53<00:12,  1.86it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  99% 2110/2132 [18:53<00:11,  1.86it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  99% 2111/2132 [18:53<00:11,  1.86it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  99% 2112/2132 [18:53<00:10,  1.86it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  99% 2113/2132 [18:54<00:10,  1.86it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  99% 2114/2132 [18:54<00:09,  1.86it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  99% 2115/2132 [18:54<00:09,  1.86it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  99% 2116/2132 [18:54<00:08,  1.87it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  99% 2117/2132 [18:54<00:08,  1.87it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  99% 2118/2132 [18:54<00:07,  1.87it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  99% 2119/2132 [18:54<00:06,  1.87it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  99% 2120/2132 [18:55<00:06,  1.87it/s, loss=0.262, v_num=]\n",
      "Epoch 0:  99% 2121/2132 [18:55<00:05,  1.87it/s, loss=0.262, v_num=]\n",
      "Epoch 0: 100% 2122/2132 [18:55<00:05,  1.87it/s, loss=0.262, v_num=]\n",
      "Epoch 0: 100% 2123/2132 [18:55<00:04,  1.87it/s, loss=0.262, v_num=]\n",
      "Epoch 0: 100% 2124/2132 [18:55<00:04,  1.87it/s, loss=0.262, v_num=]\n",
      "Epoch 0: 100% 2125/2132 [18:55<00:03,  1.87it/s, loss=0.262, v_num=]\n",
      "Epoch 0: 100% 2126/2132 [18:56<00:03,  1.87it/s, loss=0.262, v_num=]\n",
      "Epoch 0: 100% 2127/2132 [18:56<00:02,  1.87it/s, loss=0.262, v_num=]\n",
      "Epoch 0: 100% 2128/2132 [18:56<00:02,  1.87it/s, loss=0.262, v_num=]\n",
      "Epoch 0: 100% 2129/2132 [18:56<00:01,  1.87it/s, loss=0.262, v_num=]\n",
      "Epoch 0: 100% 2130/2132 [18:56<00:01,  1.87it/s, loss=0.262, v_num=]\n",
      "Epoch 0: 100% 2131/2132 [18:56<00:00,  1.87it/s, loss=0.262, v_num=]\n",
      "Epoch 0: 100% 2132/2132 [20:37<00:00,  1.72it/s, loss=0.262, v_num=]\n",
      "Epoch 0: 100% 2132/2132 [20:42<00:00,  1.72it/s, loss=0.262, v_num=]INFO:pytorch_lightning.utilities.rank_zero:Epoch 0, global step 107: 'val_loss' reached 0.28813 (best 0.28813), saving model to '/content/afrimt5_small/checkpoint.pth.ckpt' as top 1\n",
      "tcmalloc: large alloc 1171447808 bytes == 0x18ad30000 @  0x7f6f740c9615 0x5d6f4c 0x51edd1 0x51ef5b 0x5aac95 0x5d8506 0x7f6f4153e5fe 0x7f6f1ab6dc85 0x7f6f1ab681e7 0x7f6f1ab6f309 0x7f6f415510bb 0x7f6f411516af 0x5d80be 0x5d8d8c 0x4fedd4 0x4997c7 0x55d078 0x5d8941 0x4990ca 0x55cd91 0x5d8941 0x4997c7 0x5d8868 0x4990ca 0x4fd8b5 0x49abe4 0x4fd8b5 0x49abe4 0x4fd8b5 0x49abe4 0x55cd91\n",
      "tcmalloc: large alloc 1464311808 bytes == 0x126474000 @  0x7f6f740c9615 0x5d6f4c 0x51edd1 0x51ef5b 0x5aac95 0x5d8506 0x7f6f4153e5fe 0x7f6f1ab6dc85 0x7f6f1ab681e7 0x7f6f1ab6f309 0x7f6f415510bb 0x7f6f411516af 0x5d80be 0x5d8d8c 0x4fedd4 0x4997c7 0x55d078 0x5d8941 0x4990ca 0x55cd91 0x5d8941 0x4997c7 0x5d8868 0x4990ca 0x4fd8b5 0x49abe4 0x4fd8b5 0x49abe4 0x4fd8b5 0x49abe4 0x55cd91\n",
      "tcmalloc: large alloc 1830395904 bytes == 0x7f69e14a6000 @  0x7f6f740c9615 0x5d6f4c 0x51edd1 0x51ef5b 0x5aac95 0x5d8506 0x7f6f4153e5fe 0x7f6f1ab6dc85 0x7f6f1ab681e7 0x7f6f1ab6f309 0x7f6f415510bb 0x7f6f411516af 0x5d80be 0x5d8d8c 0x4fedd4 0x4997c7 0x55d078 0x5d8941 0x4990ca 0x55cd91 0x5d8941 0x4997c7 0x5d8868 0x4990ca 0x4fd8b5 0x49abe4 0x4fd8b5 0x49abe4 0x4fd8b5 0x49abe4 0x55cd91\n",
      "tcmalloc: large alloc 2288001024 bytes == 0x126474000 @  0x7f6f740c9615 0x5d6f4c 0x51edd1 0x51ef5b 0x5aac95 0x5d8506 0x7f6f4153e5fe 0x7f6f1ab6dc85 0x7f6f1ab681e7 0x7f6f1ab6f309 0x7f6f415510bb 0x7f6f411516af 0x5d80be 0x5d8d8c 0x4fedd4 0x4997c7 0x55d078 0x5d8941 0x4990ca 0x55cd91 0x5d8941 0x4997c7 0x5d8868 0x4990ca 0x4fd8b5 0x49abe4 0x4fd8b5 0x49abe4 0x4fd8b5 0x49abe4 0x55cd91\n",
      "tcmalloc: large alloc 2860007424 bytes == 0x7f6936d22000 @  0x7f6f740c9615 0x5d6f4c 0x51edd1 0x51ef5b 0x5aac95 0x5d8506 0x7f6f4153e5fe 0x7f6f1ab6dc85 0x7f6f1ab681e7 0x7f6f1ab6f309 0x7f6f415510bb 0x7f6f411516af 0x5d80be 0x5d8d8c 0x4fedd4 0x4997c7 0x55d078 0x5d8941 0x4990ca 0x55cd91 0x5d8941 0x4997c7 0x5d8868 0x4990ca 0x4fd8b5 0x49abe4 0x4fd8b5 0x49abe4 0x4fd8b5 0x49abe4 0x55cd91\n",
      "tcmalloc: large alloc 3575013376 bytes == 0x7f6861bbc000 @  0x7f6f740c9615 0x5d6f4c 0x51edd1 0x51ef5b 0x5aac95 0x5d8506 0x7f6f4153e5fe 0x7f6f1ab6dc85 0x7f6f1ab681e7 0x7f6f1ab6f309 0x7f6f415510bb 0x7f6f411516af 0x5d80be 0x5d8d8c 0x4fedd4 0x4997c7 0x55d078 0x5d8941 0x4990ca 0x55cd91 0x5d8941 0x4997c7 0x5d8868 0x4990ca 0x4fd8b5 0x49abe4 0x4fd8b5 0x49abe4 0x4fd8b5 0x49abe4 0x55cd91\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "train_data_path=\"/content/afrisent-semeval-2023/multilingual-training/SubtaskA/yo_train_new.tsv\"\n",
    "eval_data_path=\"/content/afrisent-semeval-2023/multilingual-training/SubtaskA/yo_dev_new.tsv\"\n",
    "test_data_path=\"/content/afrisent-semeval-2023/multilingual-training/SubtaskA/yo_dev_new.tsv\"\n",
    "model_name_or_path=\"masakhane/afri-byt5-base\"\n",
    "tokenizer_name_or_path=\"masakhane/afri-byt5-base\"\n",
    "max_seq_length=\"128\"\n",
    "output_dir=\"/content/afribyt5_small\"\n",
    "learning_rate=\"3e-4\"\n",
    "train_batch_size=\"4\"\n",
    "eval_batch_size=\"4\"\n",
    "num_train_epochs=\"5\"\n",
    "gradient_accumulation_steps=\"16\"\n",
    "lang=\"yoruba\"\n",
    "class_labels=\"positive,negative,neutral\"\n",
    "data_column=\"tweet\"\n",
    "target_column=\"label\"\n",
    "prompt=\"classify: \"\n",
    "\n",
    "\n",
    "!python3 /content/afriteva/classification_scripts/classification_trainer.py --train_data_path=$train_data_path \\\n",
    "    --eval_data_path=$eval_data_path \\\n",
    "    --test_data_path=$test_data_path \\\n",
    "    --model_name_or_path=$model_name_or_path \\\n",
    "    --tokenizer_name_or_path=$tokenizer_name_or_path \\\n",
    "    --output_dir=$output_dir \\\n",
    "    --max_seq_length=$max_seq_length \\\n",
    "    --train_batch_size=$train_batch_size \\\n",
    "    --eval_batch_size=$eval_batch_size \\\n",
    "    --num_train_epochs=$num_train_epochs \\\n",
    "    --gradient_accumulation_steps=$gradient_accumulation_steps \\\n",
    "    --lang=$lang \\\n",
    "    --class_labels=$class_labels \\\n",
    "    --target_column=$target_column \\\n",
    "    --data_column=$data_column \\\n",
    "    --prompt=$prompt \\\n",
    "    --learning_rate=\"3e-4\" \\\n",
    "    --weight_decay=\"0.0\" \\\n",
    "    --adam_epsilon=\"1e-8\" \\\n",
    "    --warmup_steps=\"0\" \\\n",
    "    --n_gpu=\"1\" \\\n",
    "    --fp_16=\"false\" \\\n",
    "    --max_grad_norm=\"1.0\" \\\n",
    "    --opt_level=\"O1\" \\\n",
    "    --seed=\"42\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8DuP-F73n9jP"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "env-hsd",
   "language": "python",
   "name": "env-hsd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
