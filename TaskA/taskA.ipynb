{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Track 7--ب"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Davlan/afro-xlmr-large     jhu-clsp/bernice     cardiffnlp/twitter-xlm-roberta-base-sentiment  SI2M-Lab/DarijaBERT   setu4993/LaBSE   Ammar-alhaj-ali/arabic-MARBERT-sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-06 01:00:27.530152: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-06 01:00:27.530185: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Downloading (…)lve/main/config.json: 100%|█████| 620/620 [00:00<00:00, 83.0kB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|█████| 176/176 [00:00<00:00, 28.9kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|████| 446k/446k [00:00<00:00, 741kB/s]\n",
      "Downloading (…)\"pytorch_model.bin\";: 100%|███| 498M/498M [00:12<00:00, 40.3MB/s]\n",
      "Some weights of the model checkpoint at alger-ia/dziribert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at alger-ia/dziribert and are newly initialized: ['classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "{'neutral': 0, 'positive': 1, 'negative': 2}\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 10.71ba/s]\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 22.21ba/s]\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, ID, __index_level_0__. If tweet, ID, __index_level_0__ are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/sanala/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 6077\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 950\n",
      "  Number of trainable parameters = 124443651\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msana-ltu\u001b[0m (\u001b[33mml_ltu\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.9 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/sanala/Juputer try/afrisenti/TaskA/wandb/run-20230206_010056-tu85kg97\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mvibrant-forest-352\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ml_ltu/huggingface\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ml_ltu/huggingface/runs/tu85kg97\u001b[0m\n",
      "  0%|                                                   | 0/950 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "{'loss': 0.57, 'learning_rate': 4.473684210526316e-05, 'epoch': 0.53}           \n",
      "{'loss': 0.4691, 'learning_rate': 3.9473684210526316e-05, 'epoch': 1.05}        \n",
      "{'loss': 0.2763, 'learning_rate': 3.421052631578947e-05, 'epoch': 1.58}         \n",
      "{'loss': 0.2327, 'learning_rate': 2.8947368421052634e-05, 'epoch': 2.11}        \n",
      "{'loss': 0.108, 'learning_rate': 2.368421052631579e-05, 'epoch': 2.63}          \n",
      " 53%|█████████████████████▌                   | 500/950 [02:57<02:38,  2.84it/s]Saving model checkpoint to trainerfile/checkpoint-500\n",
      "Configuration saved in trainerfile/checkpoint-500/config.json\n",
      "Model weights saved in trainerfile/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in trainerfile/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in trainerfile/checkpoint-500/special_tokens_map.json\n",
      "{'loss': 0.0874, 'learning_rate': 1.8421052631578947e-05, 'epoch': 3.16}        \n",
      "{'loss': 0.0324, 'learning_rate': 1.3157894736842106e-05, 'epoch': 3.68}        \n",
      "{'loss': 0.0266, 'learning_rate': 7.894736842105263e-06, 'epoch': 4.21}         \n",
      "{'loss': 0.0071, 'learning_rate': 2.631578947368421e-06, 'epoch': 4.74}         \n",
      "100%|█████████████████████████████████████████| 950/950 [05:38<00:00,  2.89it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 345.873, 'train_samples_per_second': 87.85, 'train_steps_per_second': 2.747, 'train_loss': 0.1910359880171324, 'epoch': 5.0}\n",
      "100%|█████████████████████████████████████████| 950/950 [05:38<00:00,  2.81it/s]\n",
      "Saving model checkpoint to alger-ia/dziribert-ma\n",
      "Configuration saved in alger-ia/dziribert-ma/config.json\n",
      "Model weights saved in alger-ia/dziribert-ma/pytorch_model.bin\n",
      "tokenizer config file saved in alger-ia/dziribert-ma/tokenizer_config.json\n",
      "Special tokens file saved in alger-ia/dziribert-ma/special_tokens_map.json\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \\ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▂▃▃▄▅▆▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▂▃▃▄▅▆▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate █▇▆▅▄▄▃▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss █▇▄▄▂▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 5.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 950\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.0071\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 3122930155338000.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.19104\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 345.873\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 87.85\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 2.747\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mvibrant-forest-352\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/ml_ltu/huggingface/runs/tu85kg97\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230206_010056-tu85kg97/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python run_train_ma.py  --model_name  'alger-ia/dziribert'  --language 'ma' --learning_rate 5e-5 --epoch 5.0  --output_dir 'alger-ia/dziribert-ma'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run_predict_ma.py --model_name 'cardiffnlp/twitter-xlm-roberta-base-sentiment' --language 'ma' --output_dir 'cardiffnlp/twitter-xlm-roberta-base-sentiment-multi' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-06 01:06:51.651398: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-06 01:06:51.651446: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2961\n",
      "  Batch size = 8\n",
      "100%|█████████████████████████████████████████| 371/371 [00:06<00:00, 56.66it/s]\n",
      "Data directory found.\n",
      "Creating submission files directory.\n"
     ]
    }
   ],
   "source": [
    "!python run_predict_ma.py --model_name 'alger-ia/dziribert' --language 'ma' --output_dir 'alger-ia/dziribert-ma' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-30 07:05:49.883494: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-30 07:05:49.883534: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2961\n",
      "  Batch size = 8\n",
      " 99%|████████████████████████████████████████▌| 367/371 [00:06<00:00, 55.21it/s]Data directory found.\n",
      "Creating submission files directory.\n",
      "100%|█████████████████████████████████████████| 371/371 [00:07<00:00, 51.75it/s]\n"
     ]
    }
   ],
   "source": [
    "!python run_predict_ma.py --model_name 'alger-ia/dziribert' --language 'ma' --output_dir 'alger-ia/dziribert-ma' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## multi-lingual trained on all data and predict on ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " setu4993/LaBSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cardiffnlp/twitter-xlm-roberta-base-sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-29 18:06:03.760135: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-29 18:06:03.760166: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Downloading: 100%|█████████████████████████████| 841/841 [00:00<00:00, 1.36MB/s]\n",
      "Downloading: 100%|█████████████████████████| 4.83M/4.83M [00:00<00:00, 14.5MB/s]\n",
      "Downloading: 100%|██████████████████████████████| 150/150 [00:00<00:00, 204kB/s]\n",
      "401 Client Error: Unauthorized for url: https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment-dz/resolve/main/config.json\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sanala/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py\", line 585, in _get_config_dict\n",
      "    resolved_config_file = cached_path(\n",
      "  File \"/home/sanala/anaconda3/lib/python3.8/site-packages/transformers/file_utils.py\", line 1846, in cached_path\n",
      "    output_path = get_from_cache(\n",
      "  File \"/home/sanala/anaconda3/lib/python3.8/site-packages/transformers/file_utils.py\", line 2050, in get_from_cache\n",
      "    _raise_for_status(r)\n",
      "  File \"/home/sanala/anaconda3/lib/python3.8/site-packages/transformers/file_utils.py\", line 1977, in _raise_for_status\n",
      "    request.raise_for_status()\n",
      "  File \"/home/sanala/anaconda3/lib/python3.8/site-packages/requests/models.py\", line 1021, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment-dz/resolve/main/config.json\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"run_predict.py\", line 78, in <module>\n",
      "    main()\n",
      "  File \"run_predict.py\", line 28, in main\n",
      "    model = AutoModelForSequenceClassification.from_pretrained(args.output_dir)\n",
      "  File \"/home/sanala/anaconda3/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\", line 424, in from_pretrained\n",
      "    config, kwargs = AutoConfig.from_pretrained(\n",
      "  File \"/home/sanala/anaconda3/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py\", line 612, in from_pretrained\n",
      "    config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
      "  File \"/home/sanala/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py\", line 537, in get_config_dict\n",
      "    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
      "  File \"/home/sanala/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py\", line 618, in _get_config_dict\n",
      "    raise EnvironmentError(\n",
      "OSError: We couldn't connect to 'https://huggingface.co/' to load this model and it looks like cardiffnlp/twitter-xlm-roberta-base-sentiment-dz is not the path to a directory conaining a config.json file.\n",
      "Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.\n"
     ]
    }
   ],
   "source": [
    "!python run_predict.py --model_name 'cardiffnlp/twitter-xlm-roberta-base-sentiment' --language 'dz' --output_dir 'cardiffnlp/twitter-xlm-roberta-base-sentiment-dz' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-29 18:06:17.410402: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-29 18:06:17.410429: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Downloading: 100%|█████████████████████████| 1.04G/1.04G [00:14<00:00, 78.0MB/s]\n",
      "  0%|                                                     | 0/2 [00:00<?, ?ba/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 23.25ba/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 80.34ba/s]\n",
      "The following columns in the training set  don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: ID, tweet, __index_level_0__.\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1651\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 52\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msana-ltu\u001b[0m (\u001b[33mml_ltu\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.7 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/sanala/Juputer try/afrisenti/TaskA/wandb/run-20221229_180650-amuci5zk\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtrainerfile\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ml_ltu/huggingface\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ml_ltu/huggingface/runs/amuci5zk\u001b[0m\n",
      "100%|███████████████████████████████████████████| 52/52 [00:07<00:00,  7.85it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 14.4858, 'train_samples_per_second': 113.974, 'train_steps_per_second': 3.59, 'train_loss': 0.7714840815617487, 'epoch': 1.0}\n",
      "100%|███████████████████████████████████████████| 52/52 [00:07<00:00,  7.08it/s]\n",
      "Saving model checkpoint to cardiffnlp/twitter-xlm-roberta-base-sentiment-dz\n",
      "Configuration saved in cardiffnlp/twitter-xlm-roberta-base-sentiment-dz/config.json\n",
      "Model weights saved in cardiffnlp/twitter-xlm-roberta-base-sentiment-dz/pytorch_model.bin\n",
      "tokenizer config file saved in cardiffnlp/twitter-xlm-roberta-base-sentiment-dz/tokenizer_config.json\n",
      "Special tokens file saved in cardiffnlp/twitter-xlm-roberta-base-sentiment-dz/special_tokens_map.json\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: / 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 52\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 46293058611702.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.77148\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 14.4858\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 113.974\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 3.59\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mtrainerfile\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/ml_ltu/huggingface/runs/amuci5zk\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20221229_180650-amuci5zk/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python run_train.py  --model_name  'cardiffnlp/twitter-xlm-roberta-base-sentiment' --language 'dz' --learning_rate 5e-5 --epoch 1.0 --output_dir 'cardiffnlp/twitter-xlm-roberta-base-sentiment-dz'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run_predict.py --model_name 'setu4993/LaBSE' --language 'dz' --output_dir 'setu4993/LaBSE-dz' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-02 14:11:17.738086: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-02 14:11:17.738119: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "100%|█████████████████████████████████████████████| 6/6 [00:00<00:00, 10.17ba/s]\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 26.67ba/s]\n",
      "The following columns in the training set  don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: tweet, ID, __index_level_0__.\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 5583\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 525\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msana-ltu\u001b[0m (\u001b[33mml_ltu\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.7 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/sanala/Juputer try/afrisenti/TaskA/wandb/run-20230102_141136-1bltzalm\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtrainerfile\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ml_ltu/huggingface\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ml_ltu/huggingface/runs/1bltzalm\u001b[0m\n",
      "{'loss': 0.749, 'learning_rate': 4.047619047619048e-05, 'epoch': 0.57}          \n",
      "{'loss': 0.5777, 'learning_rate': 3.095238095238095e-05, 'epoch': 1.14}         \n",
      "{'loss': 0.5088, 'learning_rate': 2.1428571428571428e-05, 'epoch': 1.71}        \n",
      "{'loss': 0.4091, 'learning_rate': 1.1904761904761905e-05, 'epoch': 2.29}        \n",
      "{'loss': 0.3614, 'learning_rate': 2.3809523809523808e-06, 'epoch': 2.86}        \n",
      " 95%|███████████████████████████████████████  | 500/525 [03:00<00:10,  2.40it/s]Saving model checkpoint to trainerfile/checkpoint-500\n",
      "Configuration saved in trainerfile/checkpoint-500/config.json\n",
      "Model weights saved in trainerfile/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in trainerfile/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in trainerfile/checkpoint-500/special_tokens_map.json\n",
      "100%|█████████████████████████████████████████| 525/525 [03:18<00:00,  3.14it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 205.6556, 'train_samples_per_second': 81.442, 'train_steps_per_second': 2.553, 'train_loss': 0.5151324662708101, 'epoch': 3.0}\n",
      "100%|█████████████████████████████████████████| 525/525 [03:18<00:00,  2.65it/s]\n",
      "Saving model checkpoint to cardiffnlp/twitter-xlm-roberta-base-sentiment-ma\n",
      "Configuration saved in cardiffnlp/twitter-xlm-roberta-base-sentiment-ma/config.json\n",
      "Model weights saved in cardiffnlp/twitter-xlm-roberta-base-sentiment-ma/pytorch_model.bin\n",
      "tokenizer config file saved in cardiffnlp/twitter-xlm-roberta-base-sentiment-ma/tokenizer_config.json\n",
      "Special tokens file saved in cardiffnlp/twitter-xlm-roberta-base-sentiment-ma/special_tokens_map.json\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \\ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▃▄▆██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▃▄▆██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate █▆▄▃▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss █▅▄▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 3.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 525\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.3614\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 1721440091221200.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.51513\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 205.6556\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 81.442\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 2.553\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mtrainerfile\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/ml_ltu/huggingface/runs/1bltzalm\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230102_141136-1bltzalm/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python run_train.py  --model_name  'cardiffnlp/twitter-xlm-roberta-base-sentiment' --language 'ma' --learning_rate 5e-5 --epoch 3.0 --output_dir 'cardiffnlp/twitter-xlm-roberta-base-sentiment-ma'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-02 14:15:14.719251: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-02 14:15:14.719281: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1215\n",
      "  Batch size = 8\n",
      " 97%|███████████████████████████████████████▉ | 148/152 [00:03<00:00, 47.69it/s]Data directory found.\n",
      "100%|█████████████████████████████████████████| 152/152 [00:03<00:00, 40.21it/s]\n"
     ]
    }
   ],
   "source": [
    "!python run_predict.py --model_name 'cardiffnlp/twitter-xlm-roberta-base-sentiment' --language 'ma' --output_dir 'cardiffnlp/twitter-xlm-roberta-base-sentiment-ma' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-02 11:30:28.154833: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-02 11:30:28.154888: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Some weights of the model checkpoint at Davlan/afro-xlmr-small were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at Davlan/afro-xlmr-small and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|                                                     | 0/2 [00:00<?, ?ba/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 21.11ba/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 79.37ba/s]\n",
      "The following columns in the training set  don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: tweet, ID, __index_level_0__.\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1651\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 104\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msana-ltu\u001b[0m (\u001b[33mml_ltu\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.7 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/sanala/Juputer try/afrisenti/TaskA/wandb/run-20230102_113042-1nlrub88\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtrainerfile\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ml_ltu/huggingface\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ml_ltu/huggingface/runs/1nlrub88\u001b[0m\n",
      "{'loss': 0.9002, 'learning_rate': 1.9230769230769234e-06, 'epoch': 1.92}        \n",
      " 99%|████████████████████████████████████████▌| 103/104 [00:15<00:00,  6.21it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 22.537, 'train_samples_per_second': 146.514, 'train_steps_per_second': 4.615, 'train_loss': 0.8922457030186286, 'epoch': 2.0}\n",
      "100%|█████████████████████████████████████████| 104/104 [00:15<00:00,  6.80it/s]\n",
      "Saving model checkpoint to Davlan/afro-xlmr-small-dz\n",
      "Configuration saved in Davlan/afro-xlmr-small-dz/config.json\n",
      "Model weights saved in Davlan/afro-xlmr-small-dz/pytorch_model.bin\n",
      "tokenizer config file saved in Davlan/afro-xlmr-small-dz/tokenizer_config.json\n",
      "Special tokens file saved in Davlan/afro-xlmr-small-dz/special_tokens_map.json\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 104\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.9002\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 114790422991356.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.89225\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 22.537\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 146.514\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 4.615\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mtrainerfile\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/ml_ltu/huggingface/runs/1nlrub88\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230102_113042-1nlrub88/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python run_train.py  --model_name  'Davlan/afro-xlmr-small' --language 'dz' --learning_rate 5e-5 --epoch 2.0 --output_dir 'Davlan/afro-xlmr-small-dz'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-02 13:37:27.651503: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-02 13:37:27.651536: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 414\n",
      "  Batch size = 8\n",
      " 92%|███████████████████████████████████████▋   | 48/52 [00:00<00:00, 75.52it/s]Data directory found.\n",
      "100%|███████████████████████████████████████████| 52/52 [00:01<00:00, 49.49it/s]\n"
     ]
    }
   ],
   "source": [
    "!python run_predict.py --model_name 'Davlan/afro-xlmr-small' --language 'dz' --output_dir 'Davlan/afro-xlmr-small-dz' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-02 14:05:30.396163: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-02 14:05:30.396199: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "Some weights of the model checkpoint at SI2M-Lab/DarijaBERT were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at SI2M-Lab/DarijaBERT and are newly initialized: ['classifier.weight', 'classifier.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|█████████████████████████████████████████████| 6/6 [00:00<00:00,  9.22ba/s]\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 20.06ba/s]\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: ID, tweet, __index_level_0__.\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 5583\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 525\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msana-ltu\u001b[0m (\u001b[33mml_ltu\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.7 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/sanala/Juputer try/afrisenti/TaskA/wandb/run-20230102_140546-3dg28rct\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtrainerfile\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ml_ltu/huggingface\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ml_ltu/huggingface/runs/3dg28rct\u001b[0m\n",
      "{'loss': 0.7072, 'learning_rate': 4.047619047619048e-05, 'epoch': 0.57}         \n",
      "{'loss': 0.5657, 'learning_rate': 3.095238095238095e-05, 'epoch': 1.14}         \n",
      "{'loss': 0.4769, 'learning_rate': 2.1428571428571428e-05, 'epoch': 1.71}        \n",
      "{'loss': 0.3744, 'learning_rate': 1.1904761904761905e-05, 'epoch': 2.29}        \n",
      "{'loss': 0.3329, 'learning_rate': 2.3809523809523808e-06, 'epoch': 2.86}        \n",
      " 95%|███████████████████████████████████████  | 500/525 [02:56<00:10,  2.43it/s]Saving model checkpoint to trainerfile/checkpoint-500\n",
      "Configuration saved in trainerfile/checkpoint-500/config.json\n",
      "Model weights saved in trainerfile/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in trainerfile/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in trainerfile/checkpoint-500/special_tokens_map.json\n",
      "100%|█████████████████████████████████████████| 525/525 [03:10<00:00,  3.12it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 197.3117, 'train_samples_per_second': 84.886, 'train_steps_per_second': 2.661, 'train_loss': 0.48454639071509953, 'epoch': 3.0}\n",
      "100%|█████████████████████████████████████████| 525/525 [03:10<00:00,  2.76it/s]\n",
      "Saving model checkpoint to SI2M-Lab/DarijaBERT-ma\n",
      "Configuration saved in SI2M-Lab/DarijaBERT-ma/config.json\n",
      "Model weights saved in SI2M-Lab/DarijaBERT-ma/pytorch_model.bin\n",
      "tokenizer config file saved in SI2M-Lab/DarijaBERT-ma/tokenizer_config.json\n",
      "Special tokens file saved in SI2M-Lab/DarijaBERT-ma/special_tokens_map.json\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: | 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▃▄▆██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▃▄▆██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate █▆▄▃▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss █▅▄▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 3.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 525\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.3329\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 1721440091221200.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.48455\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 197.3117\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 84.886\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 2.661\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mtrainerfile\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/ml_ltu/huggingface/runs/3dg28rct\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230102_140546-3dg28rct/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "## MA\n",
    "!python run_train.py  --model_name  'SI2M-Lab/DarijaBERT' --language 'ma' --learning_rate 5e-5 --epoch 3.0 --output_dir 'SI2M-Lab/DarijaBERT-ma'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-02 14:09:23.081185: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-02 14:09:23.081220: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1215\n",
      "  Batch size = 8\n",
      "100%|█████████████████████████████████████████| 152/152 [00:02<00:00, 53.13it/s]Data directory found.\n",
      "100%|█████████████████████████████████████████| 152/152 [00:03<00:00, 47.23it/s]\n"
     ]
    }
   ],
   "source": [
    "!python run_predict.py --model_name 'SI2M-Lab/DarijaBERT' --language 'ma' --output_dir 'SI2M-Lab/DarijaBERT-ma' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-02 13:16:09.217544: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-02 13:16:09.217579: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Some weights of the model checkpoint at Davlan/afro-xlmr-large were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at Davlan/afro-xlmr-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|█████████████████████████████████████████████| 6/6 [00:00<00:00, 18.41ba/s]\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 54.74ba/s]\n",
      "The following columns in the training set  don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, tweet, ID.\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 5583\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 525\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msana-ltu\u001b[0m (\u001b[33mml_ltu\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.7 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/sanala/Juputer try/afrisenti/TaskA/wandb/run-20230102_131628-2f8hcbc9\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtrainerfile\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ml_ltu/huggingface\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ml_ltu/huggingface/runs/2f8hcbc9\u001b[0m\n",
      "{'loss': 1.1334, 'learning_rate': 4.047619047619048e-05, 'epoch': 0.57}         \n",
      "{'loss': 1.125, 'learning_rate': 3.095238095238095e-05, 'epoch': 1.14}          \n",
      "{'loss': 1.1164, 'learning_rate': 2.1428571428571428e-05, 'epoch': 1.71}        \n",
      "{'loss': 1.1012, 'learning_rate': 1.1904761904761905e-05, 'epoch': 2.29}        \n",
      "{'loss': 1.0986, 'learning_rate': 2.3809523809523808e-06, 'epoch': 2.86}        \n",
      " 95%|███████████████████████████████████████  | 500/525 [03:42<00:14,  1.69it/s]Saving model checkpoint to trainerfile/checkpoint-500\n",
      "Configuration saved in trainerfile/checkpoint-500/config.json\n",
      "Model weights saved in trainerfile/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in trainerfile/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in trainerfile/checkpoint-500/special_tokens_map.json\n",
      "100%|█████████████████████████████████████████| 525/525 [04:09<00:00,  2.56it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 256.4851, 'train_samples_per_second': 65.302, 'train_steps_per_second': 2.047, 'train_loss': 1.1147191074916294, 'epoch': 3.0}\n",
      "100%|█████████████████████████████████████████| 525/525 [04:09<00:00,  2.11it/s]\n",
      "Saving model checkpoint to Davlan/afro-xlmr-large-ma\n",
      "Configuration saved in Davlan/afro-xlmr-large-ma/config.json\n",
      "Model weights saved in Davlan/afro-xlmr-large-ma/pytorch_model.bin\n",
      "tokenizer config file saved in Davlan/afro-xlmr-large-ma/tokenizer_config.json\n",
      "Special tokens file saved in Davlan/afro-xlmr-large-ma/special_tokens_map.json\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: - 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▃▄▆██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▃▄▆██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate █▆▄▃▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss █▆▅▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 3.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 525\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 1.0986\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 2034473106067026.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 1.11472\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 256.4851\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 65.302\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 2.047\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mtrainerfile\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/ml_ltu/huggingface/runs/2f8hcbc9\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230102_131628-2f8hcbc9/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    " !python run_train.py  --model_name  'Davlan/afro-xlmr-large' --language 'ma' --learning_rate 5e-5 --epoch 3.0 --output_dir 'Davlan/afro-xlmr-large-ma'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-02 13:21:06.732162: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-02 13:21:06.732207: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1215\n",
      "  Batch size = 8\n",
      "100%|█████████████████████████████████████████| 152/152 [00:09<00:00, 15.62it/s]Data directory found.\n",
      "100%|█████████████████████████████████████████| 152/152 [00:10<00:00, 14.57it/s]\n"
     ]
    }
   ],
   "source": [
    "!python run_predict.py --model_name 'Davlan/afro-xlmr-large' --language 'ma' --output_dir 'Davlan/afro-xlmr-large-ma' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-02 13:42:45.928539: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-02 13:42:45.928575: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Some weights of the model checkpoint at jhu-clsp/bernice were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.decoder.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at jhu-clsp/bernice and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|█████████████████████████████████████████████| 6/6 [00:00<00:00,  9.64ba/s]\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 24.90ba/s]\n",
      "The following columns in the training set  don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, ID, tweet.\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 5583\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 350\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msana-ltu\u001b[0m (\u001b[33mml_ltu\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.7 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/sanala/Juputer try/afrisenti/TaskA/wandb/run-20230102_134304-1kfipy80\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtrainerfile\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ml_ltu/huggingface\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ml_ltu/huggingface/runs/1kfipy80\u001b[0m\n",
      "  0%|                                                   | 0/350 [00:00<?, ?it/s]Traceback (most recent call last):\n",
      "  File \"run_train.py\", line 149, in <module>\n",
      "    main()\n",
      "  File \"run_train.py\", line 134, in main\n",
      "    result=trainer.train()\n",
      "  File \"/home/sanala/anaconda3/lib/python3.8/site-packages/transformers/trainer.py\", line 1365, in train\n",
      "    tr_loss_step = self.training_step(model, inputs)\n",
      "  File \"/home/sanala/anaconda3/lib/python3.8/site-packages/transformers/trainer.py\", line 1940, in training_step\n",
      "    loss = self.compute_loss(model, inputs)\n",
      "  File \"/home/sanala/anaconda3/lib/python3.8/site-packages/transformers/trainer.py\", line 1972, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"/home/sanala/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/sanala/anaconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py\", line 1204, in forward\n",
      "    outputs = self.roberta(\n",
      "  File \"/home/sanala/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/sanala/anaconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py\", line 816, in forward\n",
      "    buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n",
      "RuntimeError: The expanded size of the tensor (200) must match the existing size (130) at non-singleton dimension 1.  Target sizes: [32, 200].  Tensor sizes: [1, 130]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 1).\u001b[0m Press Control-C to abort syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mtrainerfile\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/ml_ltu/huggingface/runs/1kfipy80\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230102_134304-1kfipy80/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    " !python run_train.py  --model_name  'jhu-clsp/bernice' --language 'ma' --learning_rate 5e-5 --epoch 2.0 --output_dir 'jhu-clsp/bernice-ma'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-02 13:43:26.221892: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-02 13:43:26.221950: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "401 Client Error: Unauthorized for url: https://huggingface.co/jhu-clsp/bernice-ma/resolve/main/config.json\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sanala/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py\", line 585, in _get_config_dict\n",
      "    resolved_config_file = cached_path(\n",
      "  File \"/home/sanala/anaconda3/lib/python3.8/site-packages/transformers/file_utils.py\", line 1846, in cached_path\n",
      "    output_path = get_from_cache(\n",
      "  File \"/home/sanala/anaconda3/lib/python3.8/site-packages/transformers/file_utils.py\", line 2050, in get_from_cache\n",
      "    _raise_for_status(r)\n",
      "  File \"/home/sanala/anaconda3/lib/python3.8/site-packages/transformers/file_utils.py\", line 1977, in _raise_for_status\n",
      "    request.raise_for_status()\n",
      "  File \"/home/sanala/anaconda3/lib/python3.8/site-packages/requests/models.py\", line 1021, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/jhu-clsp/bernice-ma/resolve/main/config.json\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"run_predict.py\", line 78, in <module>\n",
      "    main()\n",
      "  File \"run_predict.py\", line 28, in main\n",
      "    model = AutoModelForSequenceClassification.from_pretrained(args.output_dir)\n",
      "  File \"/home/sanala/anaconda3/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\", line 424, in from_pretrained\n",
      "    config, kwargs = AutoConfig.from_pretrained(\n",
      "  File \"/home/sanala/anaconda3/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py\", line 612, in from_pretrained\n",
      "    config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
      "  File \"/home/sanala/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py\", line 537, in get_config_dict\n",
      "    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
      "  File \"/home/sanala/anaconda3/lib/python3.8/site-packages/transformers/configuration_utils.py\", line 618, in _get_config_dict\n",
      "    raise EnvironmentError(\n",
      "OSError: We couldn't connect to 'https://huggingface.co/' to load this model and it looks like jhu-clsp/bernice-ma is not the path to a directory conaining a config.json file.\n",
      "Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.\n"
     ]
    }
   ],
   "source": [
    "!python run_predict.py --model_name 'jhu-clsp/bernice' --language 'ma' --output_dir 'jhu-clsp/bernice-ma' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-02 14:20:55.573727: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-02 14:20:55.573767: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Downloading: 100%|██████████████████████████████| 611/611 [00:00<00:00, 554kB/s]\n",
      "Downloading: 100%|██████████████████████████████| 327/327 [00:00<00:00, 244kB/s]\n",
      "Downloading: 100%|█████████████████████████| 13.0M/13.0M [00:00<00:00, 17.7MB/s]\n",
      "Downloading: 100%|██████████████████████████████| 125/125 [00:00<00:00, 104kB/s]\n",
      "Downloading: 100%|█████████████████████████| 1.75G/1.75G [00:57<00:00, 32.9MB/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at setu4993/LaBSE and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|█████████████████████████████████████████████| 6/6 [00:00<00:00,  8.49ba/s]\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 18.74ba/s]\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, ID, tweet.\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 5583\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 525\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msana-ltu\u001b[0m (\u001b[33mml_ltu\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.7 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/sanala/Juputer try/afrisenti/TaskA/wandb/run-20230102_142216-gxr5s7tk\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtrainerfile\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ml_ltu/huggingface\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ml_ltu/huggingface/runs/gxr5s7tk\u001b[0m\n",
      "{'loss': 0.7062, 'learning_rate': 4.047619047619048e-05, 'epoch': 0.57}         \n",
      "{'loss': 0.552, 'learning_rate': 3.095238095238095e-05, 'epoch': 1.14}          \n",
      "{'loss': 0.4542, 'learning_rate': 2.1428571428571428e-05, 'epoch': 1.71}        \n",
      "{'loss': 0.3675, 'learning_rate': 1.1904761904761905e-05, 'epoch': 2.29}        \n",
      "{'loss': 0.3138, 'learning_rate': 2.3809523809523808e-06, 'epoch': 2.86}        \n",
      " 95%|███████████████████████████████████████  | 500/525 [03:14<00:11,  2.16it/s]Saving model checkpoint to trainerfile/checkpoint-500\n",
      "Configuration saved in trainerfile/checkpoint-500/config.json\n",
      "Model weights saved in trainerfile/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in trainerfile/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in trainerfile/checkpoint-500/special_tokens_map.json\n",
      "100%|█████████████████████████████████████████| 525/525 [03:41<00:00,  2.97it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 228.6633, 'train_samples_per_second': 73.247, 'train_steps_per_second': 2.296, 'train_loss': 0.4707213156563895, 'epoch': 3.0}\n",
      "100%|█████████████████████████████████████████| 525/525 [03:41<00:00,  2.37it/s]\n",
      "Saving model checkpoint to setu4993/LaBSE-ma\n",
      "Configuration saved in setu4993/LaBSE-ma/config.json\n",
      "Model weights saved in setu4993/LaBSE-ma/pytorch_model.bin\n",
      "tokenizer config file saved in setu4993/LaBSE-ma/tokenizer_config.json\n",
      "Special tokens file saved in setu4993/LaBSE-ma/special_tokens_map.json\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: - 0.470 MB of 0.470 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▃▄▆██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▃▄▆██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate █▆▄▃▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss █▅▄▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 3.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 525\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.3138\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 1721440091221200.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.47072\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 228.6633\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 73.247\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 2.296\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mtrainerfile\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/ml_ltu/huggingface/runs/gxr5s7tk\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230102_142216-gxr5s7tk/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    " !python run_train.py  --model_name  'setu4993/LaBSE' --language 'ma' --learning_rate 5e-5 --epoch 3.0 --output_dir 'setu4993/LaBSE-ma'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-02 14:26:18.445995: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-02 14:26:18.446030: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1215\n",
      "  Batch size = 8\n",
      "100%|█████████████████████████████████████████| 152/152 [00:03<00:00, 47.75it/s]Data directory found.\n",
      "Creating submission files directory.\n",
      "100%|█████████████████████████████████████████| 152/152 [00:03<00:00, 41.09it/s]\n"
     ]
    }
   ],
   "source": [
    "!python run_predict.py --model_name 'setu4993/LaBSE' --language 'ma' --output_dir 'setu4993/LaBSE-ma' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-02 13:50:57.417154: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-02 13:50:57.417188: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "Some weights of the model checkpoint at SI2M-Lab/DarijaBERT were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at SI2M-Lab/DarijaBERT and are newly initialized: ['classifier.bias', 'bert.pooler.dense.bias', 'classifier.weight', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  7.86ba/s]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 34.44ba/s]\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: ID, tweet, __index_level_0__.\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1651\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 156\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msana-ltu\u001b[0m (\u001b[33mml_ltu\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.7 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/sanala/Juputer try/afrisenti/TaskA/wandb/run-20230102_135112-1vm2swgr\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtrainerfile\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ml_ltu/huggingface\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ml_ltu/huggingface/runs/1vm2swgr\u001b[0m\n",
      "{'loss': 0.7282, 'learning_rate': 1.794871794871795e-05, 'epoch': 1.92}         \n",
      "100%|█████████████████████████████████████████| 156/156 [00:55<00:00,  3.16it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 62.7211, 'train_samples_per_second': 78.969, 'train_steps_per_second': 2.487, 'train_loss': 0.5797578493754069, 'epoch': 3.0}\n",
      "100%|█████████████████████████████████████████| 156/156 [00:55<00:00,  2.81it/s]\n",
      "Saving model checkpoint to SI2M-Lab/DarijaBERT-dz\n",
      "Configuration saved in SI2M-Lab/DarijaBERT-dz/config.json\n",
      "Model weights saved in SI2M-Lab/DarijaBERT-dz/pytorch_model.bin\n",
      "tokenizer config file saved in SI2M-Lab/DarijaBERT-dz/tokenizer_config.json\n",
      "Special tokens file saved in SI2M-Lab/DarijaBERT-dz/special_tokens_map.json\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \\ 0.010 MB of 0.478 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 3.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 156\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 2e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.7282\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 509062796096400.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.57976\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 62.7211\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 78.969\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 2.487\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mtrainerfile\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/ml_ltu/huggingface/runs/1vm2swgr\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230102_135112-1vm2swgr/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    " !python run_train.py  --model_name  'SI2M-Lab/DarijaBERT' --language 'dz' --learning_rate 5e-5 --epoch 3.0 --output_dir 'SI2M-Lab/DarijaBERT-dz'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-02 13:52:25.745983: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-02 13:52:25.746020: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 414\n",
      "  Batch size = 8\n",
      " 83%|███████████████████████████████████▌       | 43/52 [00:00<00:00, 98.87it/s]Data directory found.\n",
      "100%|███████████████████████████████████████████| 52/52 [00:00<00:00, 61.97it/s]\n"
     ]
    }
   ],
   "source": [
    "!python run_predict.py --model_name 'SI2M-Lab/DarijaBERT' --language 'dz' --output_dir 'SI2M-Lab/DarijaBERT-dz' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alger-ia/dziribert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-02 15:04:25.671721: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-02 15:04:25.671754: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "100%|█████████████████████████████████████████████| 5/5 [00:00<00:00,  9.78ba/s]\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 20.72ba/s]\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, __index_level_0__, Unnamed: 0, ID.\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4466\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 700\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msana-ltu\u001b[0m (\u001b[33mml_ltu\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.7 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/sanala/Juputer try/afrisenti/TaskA/wandb/run-20230102_150435-bkhnowiu\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtrainerfile\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ml_ltu/huggingface\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ml_ltu/huggingface/runs/bkhnowiu\u001b[0m\n",
      "{'loss': 0.5988, 'learning_rate': 4.2857142857142856e-05, 'epoch': 0.71}        \n",
      " 20%|████████▏                                | 140/700 [00:49<02:54,  3.21it/s]The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, __index_level_0__, Unnamed: 0, ID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1117\n",
      "  Batch size = 32\n",
      "\n",
      "  0%|                                                    | 0/35 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▌                                         | 2/35 [00:00<00:01, 17.09it/s]\u001b[A\n",
      " 11%|█████                                       | 4/35 [00:00<00:02, 10.74it/s]\u001b[A\n",
      " 17%|███████▌                                    | 6/35 [00:00<00:03,  9.62it/s]\u001b[A\n",
      " 23%|██████████                                  | 8/35 [00:00<00:02,  9.19it/s]\u001b[A\n",
      " 26%|███████████▎                                | 9/35 [00:00<00:02,  9.04it/s]\u001b[A\n",
      " 29%|████████████▎                              | 10/35 [00:01<00:02,  8.92it/s]\u001b[A\n",
      " 31%|█████████████▌                             | 11/35 [00:01<00:02,  8.82it/s]\u001b[A\n",
      " 34%|██████████████▋                            | 12/35 [00:01<00:02,  8.76it/s]\u001b[A\n",
      " 37%|███████████████▉                           | 13/35 [00:01<00:02,  8.69it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 14/35 [00:01<00:02,  8.65it/s]\u001b[A\n",
      " 43%|██████████████████▍                        | 15/35 [00:01<00:02,  8.62it/s]\u001b[A\n",
      " 46%|███████████████████▋                       | 16/35 [00:01<00:02,  8.60it/s]\u001b[A\n",
      " 49%|████████████████████▉                      | 17/35 [00:01<00:02,  8.59it/s]\u001b[A\n",
      " 51%|██████████████████████                     | 18/35 [00:01<00:01,  8.57it/s]\u001b[A\n",
      " 54%|███████████████████████▎                   | 19/35 [00:02<00:01,  8.52it/s]\u001b[A\n",
      " 57%|████████████████████████▌                  | 20/35 [00:02<00:01,  8.54it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 21/35 [00:02<00:01,  8.52it/s]\u001b[A\n",
      " 63%|███████████████████████████                | 22/35 [00:02<00:01,  8.51it/s]\u001b[A\n",
      " 66%|████████████████████████████▎              | 23/35 [00:02<00:01,  8.51it/s]\u001b[A\n",
      " 69%|█████████████████████████████▍             | 24/35 [00:02<00:01,  8.53it/s]\u001b[A\n",
      " 71%|██████████████████████████████▋            | 25/35 [00:02<00:01,  8.53it/s]\u001b[A\n",
      " 74%|███████████████████████████████▉           | 26/35 [00:02<00:01,  8.53it/s]\u001b[A\n",
      " 77%|█████████████████████████████████▏         | 27/35 [00:03<00:00,  8.54it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 28/35 [00:03<00:00,  8.53it/s]\u001b[A\n",
      " 83%|███████████████████████████████████▋       | 29/35 [00:03<00:00,  8.53it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▊      | 30/35 [00:03<00:00,  8.54it/s]\u001b[A\n",
      " 89%|██████████████████████████████████████     | 31/35 [00:03<00:00,  8.54it/s]\u001b[A\n",
      " 91%|███████████████████████████████████████▎   | 32/35 [00:03<00:00,  8.53it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▌  | 33/35 [00:03<00:00,  8.52it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████▊ | 34/35 [00:03<00:00,  8.50it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.49871623516082764, 'eval_f1': 0.7748554485041282, 'eval_runtime': 4.1061, 'eval_samples_per_second': 272.033, 'eval_steps_per_second': 8.524, 'epoch': 1.0}\n",
      " 20%|████████▏                                | 140/700 [00:53<02:54,  3.21it/s]\n",
      "100%|███████████████████████████████████████████| 35/35 [00:03<00:00,  8.69it/s]\u001b[A\n",
      "{'loss': 0.3638, 'learning_rate': 3.571428571428572e-05, 'epoch': 1.43}         \u001b[A\n",
      " 40%|████████████████▍                        | 280/700 [01:42<02:12,  3.17it/s]The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, __index_level_0__, Unnamed: 0, ID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1117\n",
      "  Batch size = 32\n",
      "\n",
      "  0%|                                                    | 0/35 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▌                                         | 2/35 [00:00<00:01, 17.13it/s]\u001b[A\n",
      " 11%|█████                                       | 4/35 [00:00<00:02, 10.73it/s]\u001b[A\n",
      " 17%|███████▌                                    | 6/35 [00:00<00:03,  9.58it/s]\u001b[A\n",
      " 23%|██████████                                  | 8/35 [00:00<00:02,  9.12it/s]\u001b[A\n",
      " 26%|███████████▎                                | 9/35 [00:00<00:02,  8.93it/s]\u001b[A\n",
      " 29%|████████████▎                              | 10/35 [00:01<00:02,  8.83it/s]\u001b[A\n",
      " 31%|█████████████▌                             | 11/35 [00:01<00:02,  8.75it/s]\u001b[A\n",
      " 34%|██████████████▋                            | 12/35 [00:01<00:02,  8.69it/s]\u001b[A\n",
      " 37%|███████████████▉                           | 13/35 [00:01<00:02,  8.62it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 14/35 [00:01<00:02,  8.55it/s]\u001b[A\n",
      " 43%|██████████████████▍                        | 15/35 [00:01<00:02,  8.54it/s]\u001b[A\n",
      " 46%|███████████████████▋                       | 16/35 [00:01<00:02,  8.53it/s]\u001b[A\n",
      " 49%|████████████████████▉                      | 17/35 [00:01<00:02,  8.54it/s]\u001b[A\n",
      " 51%|██████████████████████                     | 18/35 [00:02<00:01,  8.52it/s]\u001b[A\n",
      " 54%|███████████████████████▎                   | 19/35 [00:02<00:01,  8.52it/s]\u001b[A\n",
      " 57%|████████████████████████▌                  | 20/35 [00:02<00:01,  8.49it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 21/35 [00:02<00:01,  8.48it/s]\u001b[A\n",
      " 63%|███████████████████████████                | 22/35 [00:02<00:01,  8.49it/s]\u001b[A\n",
      " 66%|████████████████████████████▎              | 23/35 [00:02<00:01,  8.46it/s]\u001b[A\n",
      " 69%|█████████████████████████████▍             | 24/35 [00:02<00:01,  8.47it/s]\u001b[A\n",
      " 71%|██████████████████████████████▋            | 25/35 [00:02<00:01,  8.48it/s]\u001b[A\n",
      " 74%|███████████████████████████████▉           | 26/35 [00:02<00:01,  8.49it/s]\u001b[A\n",
      " 77%|█████████████████████████████████▏         | 27/35 [00:03<00:00,  8.49it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 28/35 [00:03<00:00,  8.50it/s]\u001b[A\n",
      " 83%|███████████████████████████████████▋       | 29/35 [00:03<00:00,  8.50it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▊      | 30/35 [00:03<00:00,  8.49it/s]\u001b[A\n",
      " 89%|██████████████████████████████████████     | 31/35 [00:03<00:00,  8.50it/s]\u001b[A\n",
      " 91%|███████████████████████████████████████▎   | 32/35 [00:03<00:00,  8.48it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▌  | 33/35 [00:03<00:00,  8.49it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████▊ | 34/35 [00:03<00:00,  8.51it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.6399452090263367, 'eval_f1': 0.7448045541393782, 'eval_runtime': 4.1261, 'eval_samples_per_second': 270.718, 'eval_steps_per_second': 8.483, 'epoch': 2.0}\n",
      " 40%|████████████████▍                        | 280/700 [01:46<02:12,  3.17it/s]\n",
      "100%|███████████████████████████████████████████| 35/35 [00:04<00:00,  8.69it/s]\u001b[A\n",
      "{'loss': 0.2734, 'learning_rate': 2.857142857142857e-05, 'epoch': 2.14}         \u001b[A\n",
      "{'loss': 0.1066, 'learning_rate': 2.1428571428571428e-05, 'epoch': 2.86}        \n",
      " 60%|████████████████████████▌                | 420/700 [02:36<01:28,  3.18it/s]The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, __index_level_0__, Unnamed: 0, ID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1117\n",
      "  Batch size = 32\n",
      "\n",
      "  0%|                                                    | 0/35 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▌                                         | 2/35 [00:00<00:01, 17.04it/s]\u001b[A\n",
      " 11%|█████                                       | 4/35 [00:00<00:02, 10.68it/s]\u001b[A\n",
      " 17%|███████▌                                    | 6/35 [00:00<00:03,  9.57it/s]\u001b[A\n",
      " 23%|██████████                                  | 8/35 [00:00<00:02,  9.14it/s]\u001b[A\n",
      " 26%|███████████▎                                | 9/35 [00:00<00:02,  8.98it/s]\u001b[A\n",
      " 29%|████████████▎                              | 10/35 [00:01<00:02,  8.85it/s]\u001b[A\n",
      " 31%|█████████████▌                             | 11/35 [00:01<00:02,  8.77it/s]\u001b[A\n",
      " 34%|██████████████▋                            | 12/35 [00:01<00:02,  8.69it/s]\u001b[A\n",
      " 37%|███████████████▉                           | 13/35 [00:01<00:02,  8.62it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 14/35 [00:01<00:02,  8.59it/s]\u001b[A\n",
      " 43%|██████████████████▍                        | 15/35 [00:01<00:02,  8.56it/s]\u001b[A\n",
      " 46%|███████████████████▋                       | 16/35 [00:01<00:02,  8.55it/s]\u001b[A\n",
      " 49%|████████████████████▉                      | 17/35 [00:01<00:02,  8.54it/s]\u001b[A\n",
      " 51%|██████████████████████                     | 18/35 [00:01<00:01,  8.53it/s]\u001b[A\n",
      " 54%|███████████████████████▎                   | 19/35 [00:02<00:01,  8.53it/s]\u001b[A\n",
      " 57%|████████████████████████▌                  | 20/35 [00:02<00:01,  8.51it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 21/35 [00:02<00:01,  8.50it/s]\u001b[A\n",
      " 63%|███████████████████████████                | 22/35 [00:02<00:01,  8.50it/s]\u001b[A\n",
      " 66%|████████████████████████████▎              | 23/35 [00:02<00:01,  8.48it/s]\u001b[A\n",
      " 69%|█████████████████████████████▍             | 24/35 [00:02<00:01,  8.46it/s]\u001b[A\n",
      " 71%|██████████████████████████████▋            | 25/35 [00:02<00:01,  8.46it/s]\u001b[A\n",
      " 74%|███████████████████████████████▉           | 26/35 [00:02<00:01,  8.47it/s]\u001b[A\n",
      " 77%|█████████████████████████████████▏         | 27/35 [00:03<00:00,  8.46it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 28/35 [00:03<00:00,  8.48it/s]\u001b[A\n",
      " 83%|███████████████████████████████████▋       | 29/35 [00:03<00:00,  8.47it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▊      | 30/35 [00:03<00:00,  8.46it/s]\u001b[A\n",
      " 89%|██████████████████████████████████████     | 31/35 [00:03<00:00,  8.47it/s]\u001b[A\n",
      " 91%|███████████████████████████████████████▎   | 32/35 [00:03<00:00,  8.48it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▌  | 33/35 [00:03<00:00,  8.47it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████▊ | 34/35 [00:03<00:00,  8.47it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.859162449836731, 'eval_f1': 0.790949142446876, 'eval_runtime': 4.1269, 'eval_samples_per_second': 270.663, 'eval_steps_per_second': 8.481, 'epoch': 3.0}\n",
      " 60%|████████████████████████▌                | 420/700 [02:40<01:28,  3.18it/s]\n",
      "100%|███████████████████████████████████████████| 35/35 [00:04<00:00,  8.67it/s]\u001b[A\n",
      "{'loss': 0.05, 'learning_rate': 1.4285714285714285e-05, 'epoch': 3.57}          \u001b[A\n",
      " 71%|█████████████████████████████▎           | 500/700 [03:08<01:19,  2.50it/s]Saving model checkpoint to trainerfile/checkpoint-500\n",
      "Configuration saved in trainerfile/checkpoint-500/config.json\n",
      "Model weights saved in trainerfile/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in trainerfile/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in trainerfile/checkpoint-500/special_tokens_map.json\n",
      " 80%|████████████████████████████████▊        | 560/700 [03:33<00:44,  3.17it/s]The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, __index_level_0__, Unnamed: 0, ID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1117\n",
      "  Batch size = 32\n",
      "\n",
      "  0%|                                                    | 0/35 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▌                                         | 2/35 [00:00<00:01, 17.12it/s]\u001b[A\n",
      " 11%|█████                                       | 4/35 [00:00<00:02, 10.79it/s]\u001b[A\n",
      " 17%|███████▌                                    | 6/35 [00:00<00:03,  9.63it/s]\u001b[A\n",
      " 23%|██████████                                  | 8/35 [00:00<00:02,  9.16it/s]\u001b[A\n",
      " 26%|███████████▎                                | 9/35 [00:00<00:02,  9.01it/s]\u001b[A\n",
      " 29%|████████████▎                              | 10/35 [00:01<00:02,  8.87it/s]\u001b[A\n",
      " 31%|█████████████▌                             | 11/35 [00:01<00:02,  8.79it/s]\u001b[A\n",
      " 34%|██████████████▋                            | 12/35 [00:01<00:02,  8.70it/s]\u001b[A\n",
      " 37%|███████████████▉                           | 13/35 [00:01<00:02,  8.64it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 14/35 [00:01<00:02,  8.61it/s]\u001b[A\n",
      " 43%|██████████████████▍                        | 15/35 [00:01<00:02,  8.57it/s]\u001b[A\n",
      " 46%|███████████████████▋                       | 16/35 [00:01<00:02,  8.55it/s]\u001b[A\n",
      " 49%|████████████████████▉                      | 17/35 [00:01<00:02,  8.55it/s]\u001b[A\n",
      " 51%|██████████████████████                     | 18/35 [00:01<00:01,  8.53it/s]\u001b[A\n",
      " 54%|███████████████████████▎                   | 19/35 [00:02<00:01,  8.48it/s]\u001b[A\n",
      " 57%|████████████████████████▌                  | 20/35 [00:02<00:01,  8.47it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 21/35 [00:02<00:01,  8.49it/s]\u001b[A\n",
      " 63%|███████████████████████████                | 22/35 [00:02<00:01,  8.50it/s]\u001b[A\n",
      " 66%|████████████████████████████▎              | 23/35 [00:02<00:01,  8.50it/s]\u001b[A\n",
      " 69%|█████████████████████████████▍             | 24/35 [00:02<00:01,  8.50it/s]\u001b[A\n",
      " 71%|██████████████████████████████▋            | 25/35 [00:02<00:01,  8.48it/s]\u001b[A\n",
      " 74%|███████████████████████████████▉           | 26/35 [00:02<00:01,  8.47it/s]\u001b[A\n",
      " 77%|█████████████████████████████████▏         | 27/35 [00:03<00:00,  8.45it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 28/35 [00:03<00:00,  8.43it/s]\u001b[A\n",
      " 83%|███████████████████████████████████▋       | 29/35 [00:03<00:00,  8.41it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▊      | 30/35 [00:03<00:00,  8.40it/s]\u001b[A\n",
      " 89%|██████████████████████████████████████     | 31/35 [00:03<00:00,  8.40it/s]\u001b[A\n",
      " 91%|███████████████████████████████████████▎   | 32/35 [00:03<00:00,  8.41it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▌  | 33/35 [00:03<00:00,  8.42it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████▊ | 34/35 [00:03<00:00,  8.44it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.150683045387268, 'eval_f1': 0.790793528505393, 'eval_runtime': 4.1323, 'eval_samples_per_second': 270.309, 'eval_steps_per_second': 8.47, 'epoch': 4.0}\n",
      " 80%|████████████████████████████████▊        | 560/700 [03:37<00:44,  3.17it/s]\n",
      "100%|███████████████████████████████████████████| 35/35 [00:04<00:00,  8.63it/s]\u001b[A\n",
      "{'loss': 0.0218, 'learning_rate': 7.142857142857143e-06, 'epoch': 4.29}         \u001b[A\n",
      "{'loss': 0.0068, 'learning_rate': 0.0, 'epoch': 5.0}                            \n",
      "100%|█████████████████████████████████████████| 700/700 [04:26<00:00,  2.75it/s]The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, __index_level_0__, Unnamed: 0, ID.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1117\n",
      "  Batch size = 32\n",
      "\n",
      "  0%|                                                    | 0/35 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▌                                         | 2/35 [00:00<00:01, 17.06it/s]\u001b[A\n",
      " 11%|█████                                       | 4/35 [00:00<00:02, 10.74it/s]\u001b[A\n",
      " 17%|███████▌                                    | 6/35 [00:00<00:03,  9.61it/s]\u001b[A\n",
      " 23%|██████████                                  | 8/35 [00:00<00:02,  9.15it/s]\u001b[A\n",
      " 26%|███████████▎                                | 9/35 [00:00<00:02,  8.98it/s]\u001b[A\n",
      " 29%|████████████▎                              | 10/35 [00:01<00:02,  8.86it/s]\u001b[A\n",
      " 31%|█████████████▌                             | 11/35 [00:01<00:02,  8.77it/s]\u001b[A\n",
      " 34%|██████████████▋                            | 12/35 [00:01<00:02,  8.71it/s]\u001b[A\n",
      " 37%|███████████████▉                           | 13/35 [00:01<00:02,  8.65it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 14/35 [00:01<00:02,  8.60it/s]\u001b[A\n",
      " 43%|██████████████████▍                        | 15/35 [00:01<00:02,  8.58it/s]\u001b[A\n",
      " 46%|███████████████████▋                       | 16/35 [00:01<00:02,  8.55it/s]\u001b[A\n",
      " 49%|████████████████████▉                      | 17/35 [00:01<00:02,  8.51it/s]\u001b[A\n",
      " 51%|██████████████████████                     | 18/35 [00:01<00:02,  8.50it/s]\u001b[A\n",
      " 54%|███████████████████████▎                   | 19/35 [00:02<00:01,  8.49it/s]\u001b[A\n",
      " 57%|████████████████████████▌                  | 20/35 [00:02<00:01,  8.49it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 21/35 [00:02<00:01,  8.49it/s]\u001b[A\n",
      " 63%|███████████████████████████                | 22/35 [00:02<00:01,  8.51it/s]\u001b[A\n",
      " 66%|████████████████████████████▎              | 23/35 [00:02<00:01,  8.50it/s]\u001b[A\n",
      " 69%|█████████████████████████████▍             | 24/35 [00:02<00:01,  8.49it/s]\u001b[A\n",
      " 71%|██████████████████████████████▋            | 25/35 [00:02<00:01,  8.50it/s]\u001b[A\n",
      " 74%|███████████████████████████████▉           | 26/35 [00:02<00:01,  8.48it/s]\u001b[A\n",
      " 77%|█████████████████████████████████▏         | 27/35 [00:03<00:00,  8.47it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 28/35 [00:03<00:00,  8.46it/s]\u001b[A\n",
      " 83%|███████████████████████████████████▋       | 29/35 [00:03<00:00,  8.48it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▊      | 30/35 [00:03<00:00,  8.47it/s]\u001b[A\n",
      " 89%|██████████████████████████████████████     | 31/35 [00:03<00:00,  8.45it/s]\u001b[A\n",
      " 91%|███████████████████████████████████████▎   | 32/35 [00:03<00:00,  8.46it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▌  | 33/35 [00:03<00:00,  8.46it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████▊ | 34/35 [00:03<00:00,  8.47it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2148743867874146, 'eval_f1': 0.7997078884193441, 'eval_runtime': 4.1278, 'eval_samples_per_second': 270.605, 'eval_steps_per_second': 8.479, 'epoch': 5.0}\n",
      "100%|█████████████████████████████████████████| 700/700 [04:30<00:00,  2.75it/s]\n",
      "100%|███████████████████████████████████████████| 35/35 [00:04<00:00,  8.63it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 278.1007, 'train_samples_per_second': 80.295, 'train_steps_per_second': 2.517, 'train_loss': 0.20302485508578164, 'epoch': 5.0}\n",
      "100%|█████████████████████████████████████████| 700/700 [04:30<00:00,  2.59it/s]\n",
      "Saving model checkpoint to alger-ia/dziribert-ma\n",
      "Configuration saved in alger-ia/dziribert-ma/config.json\n",
      "Model weights saved in alger-ia/dziribert-ma/pytorch_model.bin\n",
      "tokenizer config file saved in alger-ia/dziribert-ma/tokenizer_config.json\n",
      "Special tokens file saved in alger-ia/dziribert-ma/special_tokens_map.json\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \\ 0.010 MB of 0.010 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        eval/f1 ▅▁▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss ▁▂▅▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime ▁▆▇█▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second █▃▂▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second █▃▂▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▁▂▃▃▅▅▆▆▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▁▂▃▃▅▅▆▆▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate █▇▆▅▃▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss █▅▄▂▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        eval/f1 0.79971\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 1.21487\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 4.1278\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 270.605\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 8.479\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 5.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 700\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.0068\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 2295047897604000.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.20302\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 278.1007\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 80.295\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 2.517\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mtrainerfile\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/ml_ltu/huggingface/runs/bkhnowiu\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230102_150435-bkhnowiu/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    " !python run_train.py  --model_name  'alger-ia/dziribert' --language 'ma' --learning_rate 5e-5 --epoch 5.0 --output_dir 'alger-ia/dziribert-ma'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-02 14:39:46.738884: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-02 14:39:46.738917: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1215\n",
      "  Batch size = 8\n",
      "100%|█████████████████████████████████████████| 152/152 [00:02<00:00, 51.82it/s]Data directory found.\n",
      "100%|█████████████████████████████████████████| 152/152 [00:03<00:00, 45.64it/s]\n"
     ]
    }
   ],
   "source": [
    "!python run_predict.py --model_name 'alger-ia/dziribert' --language 'ma' --output_dir 'alger-ia/dziribert-ma' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-10 22:32:05.427683: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-10 22:32:05.427728: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at setu4993/LaBSE and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|███████████████████████████████████████████| 64/64 [00:06<00:00, 10.11ba/s]\n",
      "100%|███████████████████████████████████████████| 64/64 [00:06<00:00, 10.20ba/s]\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, tweet, ID.\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 63685\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3982\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msana-ltu\u001b[0m (\u001b[33mml_ltu\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.7 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/sanala/Juputer try/afrisenti/TaskA/wandb/run-20230110_223238-ivcmip7g\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtrainerfile\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ml_ltu/huggingface\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ml_ltu/huggingface/runs/ivcmip7g\u001b[0m\n",
      "{'loss': 0.9065, 'learning_rate': 4.874434957307886e-05, 'epoch': 0.05}         \n",
      "{'loss': 0.8223, 'learning_rate': 4.748869914615771e-05, 'epoch': 0.1}          \n",
      "{'loss': 0.8117, 'learning_rate': 4.623304871923657e-05, 'epoch': 0.15}         \n",
      "{'loss': 0.775, 'learning_rate': 4.497739829231542e-05, 'epoch': 0.2}           \n",
      "{'loss': 0.762, 'learning_rate': 4.372174786539428e-05, 'epoch': 0.25}          \n",
      " 13%|█████                                   | 500/3982 [03:13<26:37,  2.18it/s]Saving model checkpoint to trainerfile/checkpoint-500\n",
      "Configuration saved in trainerfile/checkpoint-500/config.json\n",
      "Model weights saved in trainerfile/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in trainerfile/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in trainerfile/checkpoint-500/special_tokens_map.json\n",
      "{'loss': 0.7499, 'learning_rate': 4.246609743847313e-05, 'epoch': 0.3}          \n",
      "{'loss': 0.7486, 'learning_rate': 4.121044701155198e-05, 'epoch': 0.35}         \n",
      "{'loss': 0.7327, 'learning_rate': 3.9954796584630836e-05, 'epoch': 0.4}         \n",
      "{'loss': 0.7315, 'learning_rate': 3.86991461577097e-05, 'epoch': 0.45}          \n",
      "{'loss': 0.7119, 'learning_rate': 3.744349573078855e-05, 'epoch': 0.5}          \n",
      " 25%|█████████▊                             | 1000/3982 [06:43<22:48,  2.18it/s]Saving model checkpoint to trainerfile/checkpoint-1000\n",
      "Configuration saved in trainerfile/checkpoint-1000/config.json\n",
      "Model weights saved in trainerfile/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in trainerfile/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in trainerfile/checkpoint-1000/special_tokens_map.json\n",
      "{'loss': 0.7128, 'learning_rate': 3.618784530386741e-05, 'epoch': 0.55}         \n",
      " 29%|███████████▏                           | 1136/3982 [07:52<18:12,  2.60it/s]^C\n"
     ]
    }
   ],
   "source": [
    "!python run_train_taskB.py  --model_name  'setu4993/LaBSE'  --learning_rate 5e-5 --epoch 2.0 --output_dir 'setu4993/LaBSE-multilingual'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.90      0.53      0.67      1337\n",
      "    positive       0.18      0.77      0.30       224\n",
      "    negative       0.73      0.46      0.57       438\n",
      "\n",
      "    accuracy                           0.54      1999\n",
      "   macro avg       0.61      0.59      0.51      1999\n",
      "weighted avg       0.79      0.54      0.61      1999\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.92      0.39      0.55      1337\n",
      "    positive       0.16      0.85      0.27       224\n",
      "    negative       0.71      0.44      0.54       438\n",
      "\n",
      "    accuracy                           0.45      1999\n",
      "   macro avg       0.60      0.56      0.45      1999\n",
      "weighted avg       0.79      0.45      0.51      1999\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.94      0.39      0.55      1337\n",
      "    positive       0.18      0.86      0.29       224\n",
      "    negative       0.70      0.59      0.64       438\n",
      "\n",
      "    accuracy                           0.48      1999\n",
      "   macro avg       0.61      0.61      0.49      1999\n",
      "weighted avg       0.80      0.48      0.54      1999\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.90      0.53      0.67      1337\n",
      "    positive       0.18      0.77      0.30       224\n",
      "    negative       0.73      0.46      0.57       438\n",
      "\n",
      "    accuracy                           0.54      1999\n",
      "   macro avg       0.61      0.59      0.51      1999\n",
      "weighted avg       0.79      0.54      0.61      1999\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.88      0.68      0.77      1337\n",
      "    positive       0.29      0.60      0.39       224\n",
      "    negative       0.57      0.65      0.61       438\n",
      "\n",
      "    accuracy                           0.66      1999\n",
      "   macro avg       0.58      0.64      0.59      1999\n",
      "weighted avg       0.74      0.66      0.69      1999\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.51      0.62      0.56       850\n",
      "    positive       0.58      0.54      0.56       967\n",
      "    negative       0.73      0.67      0.70      1144\n",
      "\n",
      "    accuracy                           0.61      2961\n",
      "   macro avg       0.61      0.61      0.61      2961\n",
      "weighted avg       0.62      0.61      0.61      2961\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.47      0.47      0.47       850\n",
      "    positive       0.52      0.66      0.58       967\n",
      "    negative       0.73      0.57      0.64      1144\n",
      "\n",
      "    accuracy                           0.57      2961\n",
      "   macro avg       0.57      0.57      0.56      2961\n",
      "weighted avg       0.59      0.57      0.57      2961\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.51      0.49      0.50       850\n",
      "    positive       0.56      0.64      0.60       967\n",
      "    negative       0.68      0.61      0.64      1144\n",
      "\n",
      "    accuracy                           0.59      2961\n",
      "   macro avg       0.58      0.58      0.58      2961\n",
      "weighted avg       0.59      0.59      0.59      2961\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.51      0.52      0.52       850\n",
      "    positive       0.56      0.66      0.61       967\n",
      "    negative       0.75      0.62      0.68      1144\n",
      "\n",
      "    accuracy                           0.61      2961\n",
      "   macro avg       0.61      0.60      0.60      2961\n",
      "weighted avg       0.62      0.61      0.61      2961\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.49      0.50      0.49       850\n",
      "    positive       0.50      0.58      0.54       967\n",
      "    negative       0.67      0.57      0.62      1144\n",
      "\n",
      "    accuracy                           0.55      2961\n",
      "   macro avg       0.55      0.55      0.55      2961\n",
      "weighted avg       0.56      0.55      0.56      2961\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.76      0.75      0.76      1759\n",
      "    positive       0.72      0.77      0.74      1789\n",
      "    negative       0.87      0.82      0.84      1755\n",
      "\n",
      "    accuracy                           0.78      5303\n",
      "   macro avg       0.78      0.78      0.78      5303\n",
      "weighted avg       0.78      0.78      0.78      5303\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.72      0.82      0.76      1759\n",
      "    positive       0.76      0.66      0.71      1789\n",
      "    negative       0.86      0.85      0.85      1755\n",
      "\n",
      "    accuracy                           0.78      5303\n",
      "   macro avg       0.78      0.78      0.78      5303\n",
      "weighted avg       0.78      0.78      0.78      5303\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.72      0.84      0.77      1759\n",
      "    positive       0.79      0.65      0.72      1789\n",
      "    negative       0.87      0.87      0.87      1755\n",
      "\n",
      "    accuracy                           0.79      5303\n",
      "   macro avg       0.79      0.79      0.79      5303\n",
      "weighted avg       0.79      0.79      0.79      5303\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.72      0.82      0.76      1759\n",
      "    positive       0.76      0.66      0.71      1789\n",
      "    negative       0.86      0.85      0.85      1755\n",
      "\n",
      "    accuracy                           0.78      5303\n",
      "   macro avg       0.78      0.78      0.78      5303\n",
      "weighted avg       0.78      0.78      0.78      5303\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.69      0.81      0.75      1759\n",
      "    positive       0.77      0.54      0.63      1789\n",
      "    negative       0.77      0.87      0.81      1755\n",
      "\n",
      "    accuracy                           0.74      5303\n",
      "   macro avg       0.74      0.74      0.73      5303\n",
      "weighted avg       0.74      0.74      0.73      5303\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.78      0.63      0.70       943\n",
      "    positive       0.74      0.82      0.78      1621\n",
      "    negative       0.82      0.83      0.83      1118\n",
      "\n",
      "    accuracy                           0.77      3682\n",
      "   macro avg       0.78      0.76      0.77      3682\n",
      "weighted avg       0.78      0.77      0.77      3682\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.78      0.63      0.70       943\n",
      "    positive       0.74      0.82      0.78      1621\n",
      "    negative       0.82      0.83      0.83      1118\n",
      "\n",
      "    accuracy                           0.77      3682\n",
      "   macro avg       0.78      0.76      0.77      3682\n",
      "weighted avg       0.78      0.77      0.77      3682\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.76      0.71      0.73       943\n",
      "    positive       0.76      0.83      0.79      1621\n",
      "    negative       0.85      0.79      0.82      1118\n",
      "\n",
      "    accuracy                           0.78      3682\n",
      "   macro avg       0.79      0.77      0.78      3682\n",
      "weighted avg       0.79      0.78      0.78      3682\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.78      0.63      0.70       943\n",
      "    positive       0.74      0.82      0.78      1621\n",
      "    negative       0.82      0.83      0.83      1118\n",
      "\n",
      "    accuracy                           0.77      3682\n",
      "   macro avg       0.78      0.76      0.77      3682\n",
      "weighted avg       0.78      0.77      0.77      3682\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.67      0.75      0.71       943\n",
      "    positive       0.78      0.67      0.72      1621\n",
      "    negative       0.74      0.83      0.78      1118\n",
      "\n",
      "    accuracy                           0.74      3682\n",
      "   macro avg       0.73      0.75      0.74      3682\n",
      "weighted avg       0.74      0.74      0.74      3682\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.61      0.66      0.64       981\n",
      "    positive       0.75      0.68      0.72      1616\n",
      "    negative       0.79      0.82      0.80      1918\n",
      "\n",
      "    accuracy                           0.73      4515\n",
      "   macro avg       0.72      0.72      0.72      4515\n",
      "weighted avg       0.74      0.73      0.73      4515\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.53      0.49      0.51       981\n",
      "    positive       0.69      0.56      0.62      1616\n",
      "    negative       0.70      0.83      0.76      1918\n",
      "\n",
      "    accuracy                           0.66      4515\n",
      "   macro avg       0.64      0.63      0.63      4515\n",
      "weighted avg       0.66      0.66      0.66      4515\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.61      0.68      0.64       981\n",
      "    positive       0.77      0.67      0.72      1616\n",
      "    negative       0.79      0.82      0.80      1918\n",
      "\n",
      "    accuracy                           0.74      4515\n",
      "   macro avg       0.72      0.73      0.72      4515\n",
      "weighted avg       0.74      0.74      0.74      4515\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.53      0.49      0.51       981\n",
      "    positive       0.69      0.56      0.62      1616\n",
      "    negative       0.70      0.83      0.76      1918\n",
      "\n",
      "    accuracy                           0.66      4515\n",
      "   macro avg       0.64      0.63      0.63      4515\n",
      "weighted avg       0.66      0.66      0.66      4515\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.52      0.75      0.62       981\n",
      "    positive       0.79      0.52      0.63      1616\n",
      "    negative       0.75      0.81      0.78      1918\n",
      "\n",
      "    accuracy                           0.69      4515\n",
      "   macro avg       0.69      0.69      0.67      4515\n",
      "weighted avg       0.72      0.69      0.69      4515\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.70      0.75      0.72       353\n",
      "    positive       0.24      0.18      0.21       146\n",
      "    negative       0.70      0.72      0.71       450\n",
      "\n",
      "    accuracy                           0.65       949\n",
      "   macro avg       0.55      0.55      0.55       949\n",
      "weighted avg       0.63      0.65      0.64       949\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.69      0.71      0.70       353\n",
      "    positive       0.31      0.29      0.30       146\n",
      "    negative       0.70      0.69      0.69       450\n",
      "\n",
      "    accuracy                           0.64       949\n",
      "   macro avg       0.57      0.57      0.57       949\n",
      "weighted avg       0.63      0.64      0.64       949\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.68      0.75      0.71       353\n",
      "    positive       0.34      0.29      0.31       146\n",
      "    negative       0.71      0.69      0.70       450\n",
      "\n",
      "    accuracy                           0.65       949\n",
      "   macro avg       0.58      0.58      0.58       949\n",
      "weighted avg       0.64      0.65      0.65       949\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.69      0.71      0.70       353\n",
      "    positive       0.31      0.29      0.30       146\n",
      "    negative       0.70      0.69      0.69       450\n",
      "\n",
      "    accuracy                           0.64       949\n",
      "   macro avg       0.57      0.57      0.57       949\n",
      "weighted avg       0.63      0.64      0.64       949\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.67      0.76      0.71       353\n",
      "    positive       0.30      0.21      0.25       146\n",
      "    negative       0.69      0.68      0.69       450\n",
      "\n",
      "    accuracy                           0.64       949\n",
      "   macro avg       0.55      0.55      0.55       949\n",
      "weighted avg       0.62      0.64      0.63       949\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.72      0.86      0.79      2326\n",
      "    positive       0.05      0.00      0.00       431\n",
      "    negative       0.67      0.65      0.66      1397\n",
      "\n",
      "    accuracy                           0.70      4154\n",
      "   macro avg       0.48      0.50      0.48      4154\n",
      "weighted avg       0.63      0.70      0.66      4154\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.73      0.88      0.80      2326\n",
      "    positive       0.00      0.00      0.00       431\n",
      "    negative       0.68      0.66      0.67      1397\n",
      "\n",
      "    accuracy                           0.71      4154\n",
      "   macro avg       0.47      0.51      0.49      4154\n",
      "weighted avg       0.64      0.71      0.67      4154\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.73      0.88      0.80      2326\n",
      "    positive       0.00      0.00      0.00       431\n",
      "    negative       0.68      0.66      0.67      1397\n",
      "\n",
      "    accuracy                           0.71      4154\n",
      "   macro avg       0.47      0.51      0.49      4154\n",
      "weighted avg       0.64      0.71      0.67      4154\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.73      0.88      0.80      2326\n",
      "    positive       0.00      0.00      0.00       431\n",
      "    negative       0.68      0.66      0.67      1397\n",
      "\n",
      "    accuracy                           0.71      4154\n",
      "   macro avg       0.47      0.51      0.49      4154\n",
      "weighted avg       0.64      0.71      0.67      4154\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.74      0.85      0.79      2326\n",
      "    positive       0.15      0.01      0.02       431\n",
      "    negative       0.65      0.67      0.66      1397\n",
      "\n",
      "    accuracy                           0.71      4154\n",
      "   macro avg       0.51      0.51      0.49      4154\n",
      "weighted avg       0.65      0.71      0.67      4154\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.72      0.78      0.75       475\n",
      "    positive       0.54      0.48      0.51       154\n",
      "    negative       0.76      0.71      0.73       329\n",
      "\n",
      "    accuracy                           0.71       958\n",
      "   macro avg       0.67      0.65      0.66       958\n",
      "weighted avg       0.70      0.71      0.70       958\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.72      0.78      0.75       475\n",
      "    positive       0.54      0.48      0.51       154\n",
      "    negative       0.76      0.71      0.73       329\n",
      "\n",
      "    accuracy                           0.71       958\n",
      "   macro avg       0.67      0.65      0.66       958\n",
      "weighted avg       0.70      0.71      0.70       958\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.77      0.79      0.78       475\n",
      "    positive       0.48      0.49      0.49       154\n",
      "    negative       0.79      0.75      0.77       329\n",
      "\n",
      "    accuracy                           0.73       958\n",
      "   macro avg       0.68      0.68      0.68       958\n",
      "weighted avg       0.73      0.73      0.73       958\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.72      0.78      0.75       475\n",
      "    positive       0.54      0.48      0.51       154\n",
      "    negative       0.76      0.71      0.73       329\n",
      "\n",
      "    accuracy                           0.71       958\n",
      "   macro avg       0.67      0.65      0.66       958\n",
      "weighted avg       0.70      0.71      0.70       958\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.71      0.69      0.70       475\n",
      "    positive       0.37      0.43      0.40       154\n",
      "    negative       0.68      0.66      0.67       329\n",
      "\n",
      "    accuracy                           0.64       958\n",
      "   macro avg       0.59      0.59      0.59       958\n",
      "weighted avg       0.65      0.64      0.64       958\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.48      0.59      0.53       655\n",
      "    positive       0.81      0.68      0.74      2379\n",
      "    negative       0.51      0.69      0.59       628\n",
      "\n",
      "    accuracy                           0.67      3662\n",
      "   macro avg       0.60      0.65      0.62      3662\n",
      "weighted avg       0.70      0.67      0.68      3662\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.56      0.55      0.56       655\n",
      "    positive       0.80      0.76      0.78      2379\n",
      "    negative       0.56      0.69      0.62       628\n",
      "\n",
      "    accuracy                           0.71      3662\n",
      "   macro avg       0.64      0.67      0.65      3662\n",
      "weighted avg       0.72      0.71      0.71      3662\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.53      0.65      0.58       655\n",
      "    positive       0.83      0.72      0.77      2379\n",
      "    negative       0.58      0.72      0.64       628\n",
      "\n",
      "    accuracy                           0.71      3662\n",
      "   macro avg       0.64      0.70      0.67      3662\n",
      "weighted avg       0.73      0.71      0.72      3662\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.48      0.59      0.53       655\n",
      "    positive       0.81      0.68      0.74      2379\n",
      "    negative       0.51      0.69      0.59       628\n",
      "\n",
      "    accuracy                           0.67      3662\n",
      "   macro avg       0.60      0.65      0.62      3662\n",
      "weighted avg       0.70      0.67      0.68      3662\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.49      0.57      0.53       655\n",
      "    positive       0.80      0.75      0.77      2379\n",
      "    negative       0.60      0.65      0.62       628\n",
      "\n",
      "    accuracy                           0.70      3662\n",
      "   macro avg       0.63      0.66      0.64      3662\n",
      "weighted avg       0.71      0.70      0.70      3662\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.50      0.38      0.43        80\n",
      "    positive       0.66      0.71      0.69       444\n",
      "    negative       0.47      0.44      0.45       224\n",
      "\n",
      "    accuracy                           0.59       748\n",
      "   macro avg       0.54      0.51      0.52       748\n",
      "weighted avg       0.59      0.59      0.59       748\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.46      0.35      0.40        80\n",
      "    positive       0.66      0.68      0.67       444\n",
      "    negative       0.47      0.48      0.47       224\n",
      "\n",
      "    accuracy                           0.58       748\n",
      "   macro avg       0.53      0.50      0.51       748\n",
      "weighted avg       0.58      0.58      0.58       748\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.56      0.55      0.56        80\n",
      "    positive       0.68      0.66      0.67       444\n",
      "    negative       0.46      0.50      0.48       224\n",
      "\n",
      "    accuracy                           0.60       748\n",
      "   macro avg       0.57      0.57      0.57       748\n",
      "weighted avg       0.60      0.60      0.60       748\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.50      0.38      0.43        80\n",
      "    positive       0.66      0.71      0.69       444\n",
      "    negative       0.47      0.44      0.45       224\n",
      "\n",
      "    accuracy                           0.59       748\n",
      "   macro avg       0.54      0.51      0.52       748\n",
      "weighted avg       0.59      0.59      0.59       748\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.45      0.40      0.42        80\n",
      "    positive       0.67      0.72      0.70       444\n",
      "    negative       0.50      0.46      0.48       224\n",
      "\n",
      "    accuracy                           0.61       748\n",
      "   macro avg       0.54      0.53      0.53       748\n",
      "weighted avg       0.60      0.61      0.60       748\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.71      0.61      0.66       355\n",
      "    positive       0.61      0.67      0.64       393\n",
      "    negative       0.67      0.71      0.69       278\n",
      "\n",
      "    accuracy                           0.66      1026\n",
      "   macro avg       0.67      0.66      0.66      1026\n",
      "weighted avg       0.66      0.66      0.66      1026\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.68      0.68      0.68       355\n",
      "    positive       0.64      0.67      0.66       393\n",
      "    negative       0.71      0.67      0.69       278\n",
      "\n",
      "    accuracy                           0.67      1026\n",
      "   macro avg       0.68      0.67      0.67      1026\n",
      "weighted avg       0.67      0.67      0.67      1026\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.66      0.67      0.67       355\n",
      "    positive       0.64      0.68      0.66       393\n",
      "    negative       0.75      0.66      0.70       278\n",
      "\n",
      "    accuracy                           0.67      1026\n",
      "   macro avg       0.68      0.67      0.68      1026\n",
      "weighted avg       0.68      0.67      0.67      1026\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.71      0.61      0.66       355\n",
      "    positive       0.61      0.67      0.64       393\n",
      "    negative       0.67      0.71      0.69       278\n",
      "\n",
      "    accuracy                           0.66      1026\n",
      "   macro avg       0.67      0.66      0.66      1026\n",
      "weighted avg       0.66      0.66      0.66      1026\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.66      0.62      0.64       355\n",
      "    positive       0.60      0.62      0.61       393\n",
      "    negative       0.68      0.69      0.69       278\n",
      "\n",
      "    accuracy                           0.64      1026\n",
      "   macro avg       0.65      0.65      0.65      1026\n",
      "weighted avg       0.64      0.64      0.64      1026\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.49      0.47      0.48        90\n",
      "    positive       0.28      0.26      0.27        43\n",
      "    negative       0.59      0.63      0.61       121\n",
      "\n",
      "    accuracy                           0.51       254\n",
      "   macro avg       0.45      0.45      0.45       254\n",
      "weighted avg       0.50      0.51      0.50       254\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.53      0.54      0.54        90\n",
      "    positive       0.26      0.19      0.22        43\n",
      "    negative       0.62      0.66      0.64       121\n",
      "\n",
      "    accuracy                           0.54       254\n",
      "   macro avg       0.47      0.46      0.46       254\n",
      "weighted avg       0.52      0.54      0.53       254\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.61      0.48      0.53        90\n",
      "    positive       0.14      0.02      0.04        43\n",
      "    negative       0.57      0.83      0.68       121\n",
      "\n",
      "    accuracy                           0.57       254\n",
      "   macro avg       0.44      0.45      0.42       254\n",
      "weighted avg       0.51      0.57      0.52       254\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.61      0.48      0.53        90\n",
      "    positive       0.14      0.02      0.04        43\n",
      "    negative       0.57      0.83      0.68       121\n",
      "\n",
      "    accuracy                           0.57       254\n",
      "   macro avg       0.44      0.45      0.42       254\n",
      "weighted avg       0.51      0.57      0.52       254\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.54      0.50      0.52        90\n",
      "    positive       0.30      0.28      0.29        43\n",
      "    negative       0.63      0.68      0.65       121\n",
      "\n",
      "    accuracy                           0.55       254\n",
      "   macro avg       0.49      0.49      0.49       254\n",
      "weighted avg       0.54      0.55      0.54       254\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!bash run_res.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-hsd",
   "language": "python",
   "name": "env-hsd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
