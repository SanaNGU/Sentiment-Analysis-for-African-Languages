{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import wandb\n",
    "from torch import nn\n",
    "from transformers import AdamW,AutoTokenizer,AutoModelForSequenceClassification,TrainingArguments,Trainer,DataCollatorWithPadding,get_scheduler,get_linear_schedule_with_warmup,TrainerCallback\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, Dataset, DatasetDict,ClassLabel\n",
    "checkpoint='castorini/afriberta_large'##xlm-roberta-base ##xlm-roberta-large # castorini/afriberta_large #Davlan/naija-twitter-sentiment-afriberta-large#Davlan/xlm-roberta-base-finetuned-hausa\n",
    "from transformers import DataCollatorWithPadding\n",
    "import numpy as np\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/sanala/.cache/huggingface/hub/models--castorini--afriberta_large/snapshots/e74edb9488208f8a2aeb69be4c16d179ab385564/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"castorini/afriberta_large\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 10,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 70006\n",
      "}\n",
      "\n",
      "loading file sentencepiece.bpe.model from cache at /home/sanala/.cache/huggingface/hub/models--castorini--afriberta_large/snapshots/e74edb9488208f8a2aeb69be4c16d179ab385564/sentencepiece.bpe.model\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/sanala/.cache/huggingface/hub/models--castorini--afriberta_large/snapshots/e74edb9488208f8a2aeb69be4c16d179ab385564/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/sanala/.cache/huggingface/hub/models--castorini--afriberta_large/snapshots/e74edb9488208f8a2aeb69be4c16d179ab385564/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/sanala/.cache/huggingface/hub/models--castorini--afriberta_large/snapshots/e74edb9488208f8a2aeb69be4c16d179ab385564/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"castorini/afriberta_large\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 10,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 70006\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/sanala/.cache/huggingface/hub/models--castorini--afriberta_large/snapshots/e74edb9488208f8a2aeb69be4c16d179ab385564/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"castorini/afriberta_large\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 10,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 70006\n",
      "}\n",
      "\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /home/sanala/.cache/huggingface/hub/models--castorini--afriberta_large/snapshots/e74edb9488208f8a2aeb69be4c16d179ab385564/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"castorini/afriberta_large\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 10,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"multi_label_classification\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 70006\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/sanala/.cache/huggingface/hub/models--castorini--afriberta_large/snapshots/e74edb9488208f8a2aeb69be4c16d179ab385564/pytorch_model.bin\n",
      "Some weights of the model checkpoint at castorini/afriberta_large were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at castorini/afriberta_large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "new_classifier =nn.Sequential(\n",
    "            nn.Linear(768, 64),\n",
    "           # nn.BatchNorm1d(64),\n",
    "            nn.Tanh(),\n",
    "            #nn.Dropout(0.05),\n",
    "            nn.Linear(64,3 ),\n",
    "            nn.BatchNorm1d(3))\n",
    "\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "model =AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=3, problem_type=\"multi_label_classification\").to(device)\n",
    "#model.classifier = new_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_optimizer_and_scheduler(model):\n",
    "\n",
    "    import transformers\n",
    "    opt_parameters = []    # To be passed to the optimizer (only parameters of the layers you want to update).\n",
    "    named_parameters = list(model.named_parameters()) \n",
    "        \n",
    "    # According to AAAMLP book by A. Thakur, we generally do not use any decay \n",
    "    # for bias and LayerNorm.weight layers.\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    init_lr = 3.5e-5\n",
    "    head_lr = 3.6e-5\n",
    "    lr = init_lr\n",
    "    \n",
    "    # === Pooler and regressor ======================================================  \n",
    "    \n",
    "    params_0 = [p for n,p in named_parameters if (\"pooler\" in n or \"regressor\" in n) \n",
    "                and any(nd in n for nd in no_decay)]\n",
    "    params_1 = [p for n,p in named_parameters if (\"pooler\" in n or \"regressor\" in n)\n",
    "                and not any(nd in n for nd in no_decay)]\n",
    "    \n",
    "    head_params = {\"params\": params_0, \"lr\": head_lr, \"weight_decay\": 0.0}    \n",
    "    opt_parameters.append(head_params)\n",
    "        \n",
    "    head_params = {\"params\": params_1, \"lr\": head_lr, \"weight_decay\": 0.01}    \n",
    "    opt_parameters.append(head_params)\n",
    "                \n",
    "    # === 12 Hidden layers ==========================================================\n",
    "    \n",
    "    for layer in range(11,-1,-1):        \n",
    "        params_0 = [p for n,p in named_parameters if f\"encoder.layer.{layer}.\" in n \n",
    "                    and any(nd in n for nd in no_decay)]\n",
    "        params_1 = [p for n,p in named_parameters if f\"encoder.layer.{layer}.\" in n \n",
    "                    and not any(nd in n for nd in no_decay)]\n",
    "        \n",
    "        layer_params = {\"params\": params_0, \"lr\": lr, \"weight_decay\": 0.0}\n",
    "        opt_parameters.append(layer_params)   \n",
    "                            \n",
    "        layer_params = {\"params\": params_1, \"lr\": lr, \"weight_decay\": 0.01}\n",
    "        opt_parameters.append(layer_params)       \n",
    "        \n",
    "        lr *= 0.9  #  0.9     \n",
    "        \n",
    "    # === Embeddings layer ==========================================================\n",
    "    \n",
    "    params_0 = [p for n,p in named_parameters if \"embeddings\" in n \n",
    "                and any(nd in n for nd in no_decay)]\n",
    "    params_1 = [p for n,p in named_parameters if \"embeddings\" in n\n",
    "                and not any(nd in n for nd in no_decay)]\n",
    "    \n",
    "    embed_params = {\"params\": params_0, \"lr\": lr, \"weight_decay\": 0.0} \n",
    "    opt_parameters.append(embed_params)\n",
    "        \n",
    "    embed_params = {\"params\": params_1, \"lr\": lr, \"weight_decay\": 0.01} \n",
    "    opt_parameters.append(embed_params)  \n",
    "    #num_epochs=3\n",
    "    #num_training_steps=num_epochs*len(tokenized_datasets[\"train\"])\n",
    "    \n",
    "    #scheduler = get_linear_schedule_with_warmup(transformers.AdamW(opt_parameters, lr=init_lr), num_warmup_steps=0,num_training_steps=num_training_steps)\n",
    "\n",
    "    \n",
    "    return transformers.AdamW(opt_parameters, lr=init_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-d31027c59f8267ef\n",
      "Found cached dataset csv (/home/sanala/.cache/huggingface/datasets/csv/default-d31027c59f8267ef/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3420de54ddf4d658c0a1527f3829f75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-2e3c4776c071a710\n",
      "Found cached dataset csv (/home/sanala/.cache/huggingface/datasets/csv/default-2e3c4776c071a710/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef02784ee93d47db991d2c622579801f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-2e3c4776c071a710\n",
      "Found cached dataset csv (/home/sanala/.cache/huggingface/datasets/csv/default-2e3c4776c071a710/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aab5fff2c2524fe99855641050d3460b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data=load_dataset(\"csv\", data_files='/home/sanala/Juputer try/afrisenti/TaskA/train/dz_train.tsv', delimiter=\"\\t\")\n",
    "# 10-fold cross-validation (see also next section on rounding behavior):\n",
    "# The validation datasets are each going to be 10%:\n",
    "# [0%:10%], [10%:20%], ..., [90%:100%].\n",
    "# And the training datasets are each going to be the complementary 90%:\n",
    "# [10%:100%] (for a corresponding validation set of [0%:10%]),\n",
    "# [0%:10%] + [20%:100%] (for a validation set of [10%:20%]), ...,\n",
    "# [0%:90%] (for a validation set of [90%:100%]).\n",
    "vals_ds = load_dataset(\"csv\", data_files='/home/sanala/Juputer try/afrisenti/TaskA/train/ha_train.tsv', delimiter=\"\\t\" ,split=[\n",
    "    f'train[{k}%:{k+10}%]' for k in range(0, 25, 5)\n",
    "])\n",
    "trains_ds = load_dataset(\"csv\", data_files='/home/sanala/Juputer try/afrisenti/TaskA/train/ha_train.tsv', delimiter=\"\\t\", split=[\n",
    "    f'train[:{k}%]+train[{k+10}%:]' for k in range(0, 25, 5)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trains_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ID': 'ha_train_01418',\n",
       " 'tweet': '@user Dsn shegiya can kaga bomb ðŸ’£ a yankin ku Nabi bomb da gudu',\n",
       " 'label': 'negative'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vals_ds\n",
    "dataset = DatasetDict(\n",
    "    {\n",
    "        \"train\": trains_ds,\n",
    "        \"validation\": vals_ds\n",
    "    }\n",
    ")\n",
    "dataset['train'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['ID', 'tweet', 'label', '__index_level_0__'],\n",
       "        num_rows: 1320\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['ID', 'tweet', 'label', '__index_level_0__'],\n",
       "        num_rows: 331\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_data = pd.read_csv('/home/sanala/Juputer try/afrisenti/TaskA/train/dz_train.tsv', sep='\\t')\n",
    "\n",
    "train_data = pd.read_csv('/home/sanala/Juputer try/afrisenti/TaskA/train/dz_train.tsv', delimiter=\"\\t\")\n",
    "\n",
    "train, valid = train_test_split(train_data, test_size=0.2, random_state=0, stratify=train_data[['label']])\n",
    "train.to_csv('/home/sanala/Juputer try/afrisenti/TaskA/train/dz_train_new.tsv', sep='\\t')\n",
    "valid.to_csv('/home/sanala/Juputer try/afrisenti/TaskA/train/dz_dev_new.tsv', sep='\\t')\n",
    "\n",
    "\n",
    "train = Dataset.from_pandas(train)\n",
    "valid = Dataset.from_pandas(valid)\n",
    "dataset = DatasetDict(\n",
    "    {\n",
    "        \"train\": train,\n",
    "        \"validation\": valid\n",
    "    }\n",
    ")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "def map_label2id(example):\n",
    "    example['label'] = ClassLabels.str2int(example['label'])\n",
    "    return example\n",
    "def tokenize_function(batch):\n",
    "    \"\"\"This function takes a dictionary (like the items of our dataset) and returns a new dictionary with the keys input_ids, attention_mask, and token_type_ids.\"\"\"\n",
    "    return tokenizer(batch[\"tweet\"], truncation=True)# we will not use (padding=True) because we will not pad outside patch \"\"\" \n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    import evaluate\n",
    "    metric=evaluate.load(\"f1\")\n",
    "    logits, labels = eval_pred\n",
    "    #print('value is')\n",
    "    #print (np.argmax(labels, axis=-1))\n",
    "    pres = np.argmax(logits, axis=-1)\n",
    "    labels=np.argmax(labels, axis=-1)\n",
    "    #return f1_score(labels, preds, average=\"macro\")\n",
    "    #print(pres)\n",
    "    #print(labels)\n",
    "    return metric.compute(predictions=pres, references=labels, average='micro')\n",
    "#use custum loss function \n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get('logits')\n",
    "        # compute custom loss\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([0.242,0.757]).to(device))#0.242,0.757\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "class CustomCallback(TrainerCallback):\n",
    "    \n",
    "    def __init__(self, trainer) -> None:\n",
    "        super().__init__()\n",
    "        self._trainer = trainer\n",
    "    \n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        if control.should_evaluate:\n",
    "            control_copy = deepcopy(control)\n",
    "            self._trainer.evaluate(eval_dataset=self._trainer.train_dataset, metric_key_prefix=\"train\")\n",
    "            return control_copy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6beba2823fe4f4296d29195c54ccf9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['labels', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 11337\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6a9b42084d24a329dea834aba4cc828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 11337\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4254\n",
      "  Number of trainable parameters = 125633283\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['labels', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 2835\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4254' max='4254' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4254/4254 05:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.388000</td>\n",
       "      <td>0.772159</td>\n",
       "      <td>0.419753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.360500</td>\n",
       "      <td>0.895736</td>\n",
       "      <td>0.397178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.288400</td>\n",
       "      <td>1.005994</td>\n",
       "      <td>0.402469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to trainer-file/checkpoint-500\n",
      "Configuration saved in trainer-file/checkpoint-500/config.json\n",
      "Model weights saved in trainer-file/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in trainer-file/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in trainer-file/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to trainer-file/checkpoint-1000\n",
      "Configuration saved in trainer-file/checkpoint-1000/config.json\n",
      "Model weights saved in trainer-file/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in trainer-file/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in trainer-file/checkpoint-1000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2835\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to trainer-file/checkpoint-1500\n",
      "Configuration saved in trainer-file/checkpoint-1500/config.json\n",
      "Model weights saved in trainer-file/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in trainer-file/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in trainer-file/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to trainer-file/checkpoint-2000\n",
      "Configuration saved in trainer-file/checkpoint-2000/config.json\n",
      "Model weights saved in trainer-file/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in trainer-file/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in trainer-file/checkpoint-2000/special_tokens_map.json\n",
      "Saving model checkpoint to trainer-file/checkpoint-2500\n",
      "Configuration saved in trainer-file/checkpoint-2500/config.json\n",
      "Model weights saved in trainer-file/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in trainer-file/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in trainer-file/checkpoint-2500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2835\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to trainer-file/checkpoint-3000\n",
      "Configuration saved in trainer-file/checkpoint-3000/config.json\n",
      "Model weights saved in trainer-file/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in trainer-file/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in trainer-file/checkpoint-3000/special_tokens_map.json\n",
      "Saving model checkpoint to trainer-file/checkpoint-3500\n",
      "Configuration saved in trainer-file/checkpoint-3500/config.json\n",
      "Model weights saved in trainer-file/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in trainer-file/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in trainer-file/checkpoint-3500/special_tokens_map.json\n",
      "Saving model checkpoint to trainer-file/checkpoint-4000\n",
      "Configuration saved in trainer-file/checkpoint-4000/config.json\n",
      "Model weights saved in trainer-file/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in trainer-file/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in trainer-file/checkpoint-4000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2835\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "logging_steps=100\n",
    " \n",
    "#for i in range (len(data['train'])):\n",
    "for i in range (1):\n",
    "    train_data=dataset['train']\n",
    "    df = train_data.to_pandas()\n",
    "    #labels = df['label'].unique().tolist()\n",
    "    #ClassLabels = ClassLabel(num_classes=len(labels), names=labels)\n",
    "    #train_dataset = train_data.map(map_label2id, batched=True)\n",
    "    \n",
    "    abel_enum = {k:j for j, k in enumerate(df['label'].unique())}\n",
    "    num_labels=len(abel_enum)\n",
    "    df['labels'] = df['label'].apply(lambda x: [1.0 if abel_enum[x]==i else 0.0 for i in range(num_labels)])\n",
    "    tds = Dataset.from_pandas(df)\n",
    "    #ds = DatasetDict()\n",
    "    #ds['train'] = tds\n",
    "\n",
    "    tokenized_datasets_train=tds.map(tokenize_function, batched=True,batch_size=None)\n",
    "    tokenized_datasets_train = tokenized_datasets_train.remove_columns(['ID', 'tweet','label','__index_level_0__'])#,'__index_level_0__'\n",
    "    #tokenized_datasets_train = tokenized_datasets_train.rename_column(\"label\", \"labels\")\n",
    "    print(tokenized_datasets_train)\n",
    "    tokenized_datasets_train.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "     \n",
    "    \n",
    "    \n",
    "    valid_data=dataset['validation']\n",
    "    df = valid_data.to_pandas()\n",
    "    #labels = df['label'].unique().tolist()\n",
    "    #ClassLabels = ClassLabel(num_classes=len(labels), names=labels)\n",
    "    #valid_dataset = valid_data.map(map_label2id, batched=True)\n",
    "    abel_enum = {k:j for j, k in enumerate(df['label'].unique())}\n",
    "    df['labels'] = df['label'].apply(lambda x: [1.0 if abel_enum[x]==i else 0.0 for i in range(num_labels)])\n",
    "    vds = Dataset.from_pandas(df)\n",
    "    #ds['validation'] = vds\n",
    "    tokenized_datasets_valid = vds.map(tokenize_function, batched=True,batch_size=None)\n",
    "    tokenized_datasets_valid = tokenized_datasets_valid.remove_columns(['ID', 'tweet','label','__index_level_0__'])\n",
    "    #tokenized_datasets_valid = tokenized_datasets_valid.rename_column(\"label\", \"labels\")\n",
    "    print(tokenized_datasets_valid)\n",
    "\n",
    "    tokenized_datasets_valid.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])#,'token_type_ids'\n",
    "    \n",
    "    batch_size=8\n",
    "    #logging_steps = len(tokenized_datasets[\"train\"]) // batch_size\n",
    "    logging_steps=100\n",
    "    training_arg=TrainingArguments('trainer-file', evaluation_strategy=\"epoch\",num_train_epochs=3,logging_steps=logging_steps,learning_rate=5e-6,per_device_train_batch_size=batch_size,per_device_eval_batch_size=batch_size,weight_decay=0.01)#warmup_steps=50,)#lr_scheduler_type=\"cosine\")# directory where the trained model will be saved #or we can use (push_to_hub=True) in the TrainingArguments if we want to automatically upload your model to the Hub\n",
    "    trainer=Trainer(model,args=training_arg,train_dataset=tokenized_datasets_train,\n",
    "                    eval_dataset=tokenized_datasets_valid,\n",
    "                    data_collator=data_collator,tokenizer=tokenizer,\n",
    "                    compute_metrics=compute_metrics)\n",
    "    result=trainer.train()\n",
    "\n",
    "    \n",
    "    \n",
    "    #trainer=Trainer(model,args=training_arg,train_dataset=tokenized_datasets_train,eval_dataset=tokenized_datasets_valid,data_collator=data_collator,tokenizer=tokenizer,compute_metrics=compute_metrics)\n",
    "    #optim_scheduler = create_optimizer_and_scheduler(trainer.model) \n",
    "    #trainer.optimizer = optim_scheduler\n",
    "    #trainer.add_callback(CustomCallback(trainer)) \n",
    "    results.append(result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-hsd",
   "language": "python",
   "name": "env-hsd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
