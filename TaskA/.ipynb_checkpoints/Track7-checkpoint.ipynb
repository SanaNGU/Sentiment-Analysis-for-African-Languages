{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import wandb\n",
    "from torch import nn\n",
    "from transformers import AdamW,AutoTokenizer,AutoModelForSequenceClassification,TrainingArguments,Trainer,DataCollatorWithPadding,get_scheduler,get_linear_schedule_with_warmup,TrainerCallback\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, Dataset, DatasetDict,ClassLabel\n",
    "checkpoint='aubmindlab/bert-base-arabertv02-twitter'#UBC-NLP/MARBERTv2.  #aubmindlab/bert-base-arabertv02-twitter #alger-ia/dziribert #asafaya/bert-base-arabic#aubmindlab/bert-base-arabertv02-twitter\n",
    "from transformers import DataCollatorWithPadding\n",
    "import numpy as np\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at /home/sanala/.cache/huggingface/hub/models--aubmindlab--bert-base-arabertv02-twitter/snapshots/09ab8275bec8c26954a42a91a77b8dd68638834f/vocab.txt\n",
      "loading file tokenizer.json from cache at /home/sanala/.cache/huggingface/hub/models--aubmindlab--bert-base-arabertv02-twitter/snapshots/09ab8275bec8c26954a42a91a77b8dd68638834f/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/sanala/.cache/huggingface/hub/models--aubmindlab--bert-base-arabertv02-twitter/snapshots/09ab8275bec8c26954a42a91a77b8dd68638834f/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/sanala/.cache/huggingface/hub/models--aubmindlab--bert-base-arabertv02-twitter/snapshots/09ab8275bec8c26954a42a91a77b8dd68638834f/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/sanala/.cache/huggingface/hub/models--aubmindlab--bert-base-arabertv02-twitter/snapshots/09ab8275bec8c26954a42a91a77b8dd68638834f/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"aubmindlab/bert-base-arabertv02-twitter\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"multi_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/sanala/.cache/huggingface/hub/models--aubmindlab--bert-base-arabertv02-twitter/snapshots/09ab8275bec8c26954a42a91a77b8dd68638834f/pytorch_model.bin\n",
      "Some weights of the model checkpoint at aubmindlab/bert-base-arabertv02-twitter were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv02-twitter and are newly initialized: ['classifier.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "new_classifier =nn.Sequential(\n",
    "            nn.Linear(768, 64),\n",
    "           # nn.BatchNorm1d(64),\n",
    "            nn.Tanh(),\n",
    "            #nn.Dropout(0.05),\n",
    "            nn.Linear(64, 2),\n",
    "            nn.BatchNorm1d(2))\n",
    "\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "model =AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=3, problem_type=\"multi_label_classification\").to(device)\n",
    "#model.classifier = new_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_optimizer_and_scheduler(model):\n",
    "\n",
    "    import transformers\n",
    "    opt_parameters = []    # To be passed to the optimizer (only parameters of the layers you want to update).\n",
    "    named_parameters = list(model.named_parameters()) \n",
    "        \n",
    "    # According to AAAMLP book by A. Thakur, we generally do not use any decay \n",
    "    # for bias and LayerNorm.weight layers.\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    init_lr = 3.5e-5\n",
    "    head_lr = 3.6e-5\n",
    "    lr = init_lr\n",
    "    \n",
    "    # === Pooler and regressor ======================================================  \n",
    "    \n",
    "    params_0 = [p for n,p in named_parameters if (\"pooler\" in n or \"regressor\" in n) \n",
    "                and any(nd in n for nd in no_decay)]\n",
    "    params_1 = [p for n,p in named_parameters if (\"pooler\" in n or \"regressor\" in n)\n",
    "                and not any(nd in n for nd in no_decay)]\n",
    "    \n",
    "    head_params = {\"params\": params_0, \"lr\": head_lr, \"weight_decay\": 0.0}    \n",
    "    opt_parameters.append(head_params)\n",
    "        \n",
    "    head_params = {\"params\": params_1, \"lr\": head_lr, \"weight_decay\": 0.01}    \n",
    "    opt_parameters.append(head_params)\n",
    "                \n",
    "    # === 12 Hidden layers ==========================================================\n",
    "    \n",
    "    for layer in range(11,-1,-1):        \n",
    "        params_0 = [p for n,p in named_parameters if f\"encoder.layer.{layer}.\" in n \n",
    "                    and any(nd in n for nd in no_decay)]\n",
    "        params_1 = [p for n,p in named_parameters if f\"encoder.layer.{layer}.\" in n \n",
    "                    and not any(nd in n for nd in no_decay)]\n",
    "        \n",
    "        layer_params = {\"params\": params_0, \"lr\": lr, \"weight_decay\": 0.0}\n",
    "        opt_parameters.append(layer_params)   \n",
    "                            \n",
    "        layer_params = {\"params\": params_1, \"lr\": lr, \"weight_decay\": 0.01}\n",
    "        opt_parameters.append(layer_params)       \n",
    "        \n",
    "        lr *= 0.9  #  0.9     \n",
    "        \n",
    "    # === Embeddings layer ==========================================================\n",
    "    \n",
    "    params_0 = [p for n,p in named_parameters if \"embeddings\" in n \n",
    "                and any(nd in n for nd in no_decay)]\n",
    "    params_1 = [p for n,p in named_parameters if \"embeddings\" in n\n",
    "                and not any(nd in n for nd in no_decay)]\n",
    "    \n",
    "    embed_params = {\"params\": params_0, \"lr\": lr, \"weight_decay\": 0.0} \n",
    "    opt_parameters.append(embed_params)\n",
    "        \n",
    "    embed_params = {\"params\": params_1, \"lr\": lr, \"weight_decay\": 0.01} \n",
    "    opt_parameters.append(embed_params)  \n",
    "    #num_epochs=3\n",
    "    #num_training_steps=num_epochs*len(tokenized_datasets[\"train\"])\n",
    "    \n",
    "    #scheduler = get_linear_schedule_with_warmup(transformers.AdamW(opt_parameters, lr=init_lr), num_warmup_steps=0,num_training_steps=num_training_steps)\n",
    "\n",
    "    \n",
    "    return transformers.AdamW(opt_parameters, lr=init_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-620b2006dfaf3785\n",
      "Found cached dataset csv (/home/sanala/.cache/huggingface/datasets/csv/default-620b2006dfaf3785/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7611e635b5e845fdac6a3111cecfee91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-620b2006dfaf3785\n",
      "Found cached dataset csv (/home/sanala/.cache/huggingface/datasets/csv/default-620b2006dfaf3785/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bcd98b31bc642ac8bc3c5329e5f3d35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-620b2006dfaf3785\n",
      "Found cached dataset csv (/home/sanala/.cache/huggingface/datasets/csv/default-620b2006dfaf3785/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3f2b7ecd2174a4187547ef3ed457381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data=load_dataset(\"csv\", data_files='/home/sanala/Juputer try/afrisenti/TaskA/train/dz_train.tsv', delimiter=\"\\t\")\n",
    "# 10-fold cross-validation (see also next section on rounding behavior):\n",
    "# The validation datasets are each going to be 10%:\n",
    "# [0%:10%], [10%:20%], ..., [90%:100%].\n",
    "# And the training datasets are each going to be the complementary 90%:\n",
    "# [10%:100%] (for a corresponding validation set of [0%:10%]),\n",
    "# [0%:10%] + [20%:100%] (for a validation set of [10%:20%]), ...,\n",
    "# [0%:90%] (for a validation set of [90%:100%]).\n",
    "vals_ds = load_dataset(\"csv\", data_files='/home/sanala/Juputer try/afrisenti/TaskA/train/dz_train.tsv', delimiter=\"\\t\" ,split=[\n",
    "    f'train[{k}%:{k+10}%]' for k in range(0, 25, 5)\n",
    "])\n",
    "trains_ds = load_dataset(\"csv\", data_files='/home/sanala/Juputer try/afrisenti/TaskA/train/dz_train.tsv', delimiter=\"\\t\", split=[\n",
    "    f'train[:{k}%]+train[{k+10}%:]' for k in range(0, 25, 5)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trains_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ID': 'dz_train_00166',\n",
       " 'tweet': '@user @user @user @user @user طبعا لسنا إخوة لا بالعرق ولا بالشكل ولا بالخريطة شاج… @user',\n",
       " 'label': 'negative'}"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vals_ds\n",
    "dataset = DatasetDict(\n",
    "    {\n",
    "        \"train\": trains_ds,\n",
    "        \"validation\": vals_ds\n",
    "    }\n",
    ")\n",
    "dataset['train'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['ID', 'tweet', 'label', '__index_level_0__'],\n",
       "        num_rows: 2508\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['ID', 'tweet', 'label', '__index_level_0__'],\n",
       "        num_rows: 628\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_data = pd.read_csv('/home/sanala/Juputer try/afrisenti/TaskA/train/dz_train.tsv', sep='\\t')\n",
    "\n",
    "train_data = pd.read_csv('/home/sanala/Juputer try/afrisenti/TaskA/train/dz_train_com.csv')\n",
    "\n",
    "train, valid = train_test_split(train_data, test_size=0.2, random_state=0, stratify=train_data[['label']])\n",
    "train = Dataset.from_pandas(train)\n",
    "valid = Dataset.from_pandas(valid)\n",
    "dataset = DatasetDict(\n",
    "    {\n",
    "        \"train\": train,\n",
    "        \"validation\": valid\n",
    "    }\n",
    ")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "def map_label2id(example):\n",
    "    example['label'] = ClassLabels.str2int(example['label'])\n",
    "    return example\n",
    "def tokenize_function(batch):\n",
    "    \"\"\"This function takes a dictionary (like the items of our dataset) and returns a new dictionary with the keys input_ids, attention_mask, and token_type_ids.\"\"\"\n",
    "    return tokenizer(batch[\"tweet\"], truncation=True)# we will not use (padding=True) because we will not pad outside patch \"\"\" \n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    import evaluate\n",
    "    metric=evaluate.load(\"f1\")\n",
    "    logits, labels = eval_pred\n",
    "    #print('value is')\n",
    "    #print (np.argmax(labels, axis=-1))\n",
    "    pres = np.argmax(logits, axis=-1)\n",
    "    labels=np.argmax(labels, axis=-1)\n",
    "    #return f1_score(labels, preds, average=\"macro\")\n",
    "    #print(pres)\n",
    "    #print(labels)\n",
    "    return metric.compute(predictions=pres, references=labels, average='macro')\n",
    "#use custum loss function \n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get('logits')\n",
    "        # compute custom loss\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([0.242,0.757]).to(device))#0.242,0.757\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "class CustomCallback(TrainerCallback):\n",
    "    \n",
    "    def __init__(self, trainer) -> None:\n",
    "        super().__init__()\n",
    "        self._trainer = trainer\n",
    "    \n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        if control.should_evaluate:\n",
    "            control_copy = deepcopy(control)\n",
    "            self._trainer.evaluate(eval_dataset=self._trainer.train_dataset, metric_key_prefix=\"train\")\n",
    "            return control_copy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a9f8aa21be94cbabf2ef17dd148592d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 2508\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80c243d833d54943a0e51d35dce29fa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/sanala/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2508\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 942\n",
      "  Number of trainable parameters = 135195651\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 628\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='942' max='942' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [942/942 01:24, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.456100</td>\n",
       "      <td>0.593295</td>\n",
       "      <td>0.362808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.324300</td>\n",
       "      <td>0.706652</td>\n",
       "      <td>0.341631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.250800</td>\n",
       "      <td>0.744837</td>\n",
       "      <td>0.353328</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 628\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to trainer-file/checkpoint-500\n",
      "Configuration saved in trainer-file/checkpoint-500/config.json\n",
      "Model weights saved in trainer-file/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in trainer-file/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in trainer-file/checkpoint-500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 628\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 628\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "batch_size=8\n",
    "logging_steps=100\n",
    "training_arg=TrainingArguments('trainer-file', evaluation_strategy=\"epoch\",num_train_epochs=3,logging_steps=logging_steps,run_name=\"bert-base-Trainer\",learning_rate=1e-5,per_device_train_batch_size=batch_size, per_device_eval_batch_size=batch_size, weight_decay=0.01),#warmup_steps=50,)#lr_scheduler_type=\"cosine\")# directory where the trained model will be saved #or we can use (push_to_hub=True) in the TrainingArguments if we want to automatically upload your model to the Hub\n",
    "\n",
    "#for i in range (len(data['train'])):\n",
    "for i in range (1):\n",
    "    train_data=dataset['train']\n",
    "    df = train_data.to_pandas()\n",
    "    #labels = df['label'].unique().tolist()\n",
    "    #ClassLabels = ClassLabel(num_classes=len(labels), names=labels)\n",
    "    #train_dataset = train_data.map(map_label2id, batched=True)\n",
    "    abel_enum = {k:j for j, k in enumerate(df['label'].unique())}\n",
    "    df['labels'] = df['label'].apply(lambda x: [1.0 if abel_enum[x]==i else 0.0 for i in range(num_labels)])\n",
    "    tds = Dataset.from_pandas(df)\n",
    "    #ds = DatasetDict()\n",
    "    #ds['train'] = tds\n",
    "\n",
    "    tokenized_datasets_train=tds.map(tokenize_function, batched=True,batch_size=None)\n",
    "    tokenized_datasets_train = tokenized_datasets_train.remove_columns(['ID', 'tweet','label','__index_level_0__'])\n",
    "    #tokenized_datasets_train = tokenized_datasets_train.rename_column(\"label\", \"labels\")\n",
    "    print(tokenized_datasets_train)\n",
    "    tokenized_datasets_train.set_format('torch', columns=['input_ids', 'attention_mask', 'labels','token_type_ids'])\n",
    "     \n",
    "    \n",
    "    \n",
    "    valid_data=dataset['validation']\n",
    "    df = valid_data.to_pandas()\n",
    "    #labels = df['label'].unique().tolist()\n",
    "    #ClassLabels = ClassLabel(num_classes=len(labels), names=labels)\n",
    "    #valid_dataset = valid_data.map(map_label2id, batched=True)\n",
    "    abel_enum = {k:j for j, k in enumerate(df['label'].unique())}\n",
    "    df['labels'] = df['label'].apply(lambda x: [1.0 if abel_enum[x]==i else 0.0 for i in range(num_labels)])\n",
    "    vds = Dataset.from_pandas(df)\n",
    "    #ds['validation'] = vds\n",
    "    tokenized_datasets_valid = vds.map(tokenize_function, batched=True,batch_size=None)\n",
    "    tokenized_datasets_valid = tokenized_datasets_valid.remove_columns(['ID', 'tweet','label','__index_level_0__'])\n",
    "    #tokenized_datasets_valid = tokenized_datasets_valid.rename_column(\"label\", \"labels\")\n",
    "    print(tokenized_datasets_valid)\n",
    "\n",
    "    tokenized_datasets_valid.set_format('torch', columns=['input_ids', 'attention_mask', 'labels','token_type_ids'])\n",
    "    \n",
    "    batch_size=8\n",
    "    #logging_steps = len(tokenized_datasets[\"train\"]) // batch_size\n",
    "    logging_steps=100\n",
    "    training_arg=TrainingArguments('trainer-file', evaluation_strategy=\"epoch\",num_train_epochs=3,logging_steps=logging_steps,learning_rate=1e-5,per_device_train_batch_size=8,per_device_eval_batch_size=8,weight_decay=0.01)#warmup_steps=50,)#lr_scheduler_type=\"cosine\")# directory where the trained model will be saved #or we can use (push_to_hub=True) in the TrainingArguments if we want to automatically upload your model to the Hub\n",
    "    trainer=Trainer(model,args=training_arg,train_dataset=tokenized_datasets_train,\n",
    "                    eval_dataset=tokenized_datasets_valid,\n",
    "                    data_collator=data_collator,tokenizer=tokenizer,\n",
    "                    compute_metrics=compute_metrics)\n",
    "    result=trainer.train()\n",
    "\n",
    "    \n",
    "    \n",
    "    #trainer=Trainer(model,args=training_arg,train_dataset=tokenized_datasets_train,eval_dataset=tokenized_datasets_valid,data_collator=data_collator,tokenizer=tokenizer,compute_metrics=compute_metrics)\n",
    "    #optim_scheduler = create_optimizer_and_scheduler(trainer.model) \n",
    "    #trainer.optimizer = optim_scheduler\n",
    "    #trainer.add_callback(CustomCallback(trainer)) \n",
    "    results.append(result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TrainOutput(global_step=942, training_loss=0.4137328901108663, metrics={'train_runtime': 84.9817, 'train_samples_per_second': 88.537, 'train_steps_per_second': 11.085, 'total_flos': 135365686700328.0, 'train_loss': 0.4137328901108663, 'epoch': 3.0})]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-620b2006dfaf3785\n",
      "Found cached dataset csv (/home/sanala/.cache/huggingface/datasets/csv/default-620b2006dfaf3785/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "Loading cached split indices for dataset at /home/sanala/.cache/huggingface/datasets/csv/default-620b2006dfaf3785/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-98b2e84d569795e7.arrow and /home/sanala/.cache/huggingface/datasets/csv/default-620b2006dfaf3785/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-6549ca2902ddd4ea.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['ID', 'tweet', 'label'],\n",
      "        num_rows: 1485\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['ID', 'tweet', 'label'],\n",
      "        num_rows: 166\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data=load_dataset(\"csv\", data_files='/home/sanala/Juputer try/afrisenti/TaskA/train/dz_train.tsv', delimiter=\"\\t\", split=\"train\")\n",
    "splits = data.train_test_split(test_size=0.1, seed=2022)  # sklearn syntax\n",
    "print(splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neutral': 0, 'negative': 1, 'positive': 2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['ID', 'tweet', 'label', 'labels'],\n",
       "    num_rows: 1485\n",
       "})"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = splits[\"train\"].to_pandas()\n",
    "abel_enum = {k:j for j, k in enumerate(df['label'].unique())}\n",
    "print(abel_enum)\n",
    "num_labels = len(abel_enum)\n",
    "\n",
    "df['labels'] = df['label'].apply(lambda x: [1.0 if abel_enum[x]==i else 0.0 for i in range(num_labels)])\n",
    "tds = Dataset.from_pandas(df)\n",
    "tds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ID': 'dz_train_01047',\n",
       " 'tweet': 'هاذي زعما تكمونتيلي 😂 @user',\n",
       " 'label': 'neutral',\n",
       " 'labels': [1.0, 0.0, 0.0]}"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Creating a ClassLabel Object\n",
    "df = splits[\"train\"].to_pandas()\n",
    "labels = df['label'].unique().tolist()\n",
    "ClassLabels = ClassLabel(num_classes=len(labels), names=labels)\n",
    "\n",
    "\n",
    "\n",
    "# Mapping Labels to IDs\n",
    "def map_label2id(example):\n",
    "    example['label'] = ClassLabels.str2int(example['label'])\n",
    "    return example\n",
    "\n",
    "dataset = splits.map(map_label2id, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets_train = splits['train'].map(tokenize_function, batched=True,batch_size=None)\n",
    "tokenized_datasets_valid = splits['test'].map(tokenize_function, batched=True,batch_size=None)\n",
    "tokenized_datasets_train = tokenized_datasets_train.remove_columns(['ID', 'tweet'])\n",
    "tokenized_datasets_valid = tokenized_datasets_valid.remove_columns(['ID', 'tweet'])\n",
    "tokenized_datasets_train = tokenized_datasets_train.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets_valid = tokenized_datasets_valid.rename_column(\"label\", \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=8\n",
    "#logging_steps = len(tokenized_datasets[\"train\"]) // batch_size\n",
    "logging_steps=100\n",
    "training_arg=TrainingArguments('trainer-file', evaluation_strategy=\"epoch\",num_train_epochs=2,logging_steps=logging_steps,warmup_steps=50,)#lr_scheduler_type=\"cosine\")# directory where the trained model will be saved #or we can use (push_to_hub=True) in the TrainingArguments if we want to automatically upload your model to the Hub\n",
    "trainer=Trainer(model,args=training_arg,train_dataset=tokenized_datasets_train,\n",
    "                eval_dataset=tokenized_datasets_valid,\n",
    "                data_collator=data_collator,tokenizer=tokenizer,\n",
    "                compute_metrics=compute_metrics)\n",
    "result=trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.classification import ClassificationModel,MultiLabelClassificationModel\n",
    "    from sklearn.model_selection import KFold\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    import pandas as pd\n",
    "\n",
    "\n",
    "    # Dataset\n",
    "    dataset = [[\"Example sentence belonging to class 1\", 1],\n",
    "                         [\"Example sentence belonging to class 0\", 0],\n",
    "                         [\"Example eval sentence belonging to class 1\", 1],\n",
    "                         [\"Example eval sentence belonging to class 0\", 0]]\n",
    "    train_data = pd.DataFrame(dataset)\n",
    "\n",
    "    # prepare cross validation\n",
    "    n=5\n",
    "    kf = KFold(n_splits=n, random_state=seed, shuffle=True)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for train_index, val_index in kf.split(train_data):\n",
    "         # splitting Dataframe (dataset not included)\n",
    "        train_df = train_data.iloc[train_index]\n",
    "        val_df = train_data.iloc[val_index]\n",
    "        # Defining Model\n",
    "        model = MultiLabelClassificationModel('bert', 'bert-base-uncased')\n",
    "         # train the model\n",
    "        model.train_model(train_df)\n",
    "        # validate the model\n",
    "        result, model_outputs, wrong_predictions = model.eval_model(val_df, acc=accuracy_score)\n",
    "        print(result['acc'])\n",
    "         # append model score\n",
    "        results.append(result['acc'])\n",
    "\n",
    "\n",
    "    print(\"results\",results)\n",
    "    print(f\"Mean-Precision: {sum(results) / len(results)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-hsd",
   "language": "python",
   "name": "env-hsd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
